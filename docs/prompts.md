PROMPT 1 (Q7) — Bariery adopcji i strategie (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy analityk badań rynku technologii dla usług profesjonalnych (Polska), ekspert zarządzania zmianą w zawodach regulowanych, specjalista zaufania do AI i wdrożeń pilotażowych, konsultant ds. ryzyk regulacyjnych (RODO, tajemnica zawodowa doradcy podatkowego, odpowiedzialność).
- Domena biznesowa: adopcja narzędzi AI w polskich usługach doradztwa podatkowego (doradcy podatkowi, kancelarie doradztwa podatkowego), z naciskiem na bariery zaufania, ryzyka prawne, poufność, integrację w workflow i projekt pilota.
- Cel strategiczny: zaprojektować strategię adopcji i plan pilota (60–90 dni) dla Polish Tax Advisor GPT, minimalizując ryzyka (poufność, odpowiedzialność, zgodność) oraz maksymalizując szybkość i trwałość wdrożenia w segmencie sceptycznych profesjonalistów.
- Aktualny stan wiedzy: wcześniejsze prace (Tiers 1–4) zakończone; obecny etap to pogłębione doprecyzowanie barier, ich skali oraz skutecznych strategii przełamywania w realiach Polski.
- Luki rynkowe i szansa: brak klarownych, udokumentowanych i „auditowalnych” praktyk wdrażania asystentów AI w doradztwie podatkowym w Polsce; możliwość zbudowania zaufania poprzez dowody, mechanizmy kontroli oraz partnerstwa branżowe.
- Wpływ na decyzje: wyniki mają bezpośrednio zasilić strategię produktu i go-to-market: komunikacja, onboarding, dowody wartości, compliance-by-design, konstrukcja pilota i kryteria sukcesu.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Nie opieraj wniosków na źródłach opublikowanych po tej dacie; jeśli je znajdziesz, oznacz jako „poza zakresem (po cutoff)” i nie używaj do kluczowych tez.
- Preferuj dowody i źródła polskie. Gdy brakuje danych PL, użyj analogów UE/świat i jednoznacznie je oznacz jako analog (wraz z krótkim uzasadnieniem transferowalności lub jej braku).

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie strategiczne: jakie są główne bariery adopcji narzędzi AI przez polskich doradców podatkowych i jakie strategie (komunikacja, proof points, pilotaże, mechanizmy zgodności) je przełamują?
- Hipotezy robocze do weryfikacji (potwierdź lub obal dowodami):
  - H1: obawy o odpowiedzialność zawodową są barierą nr 1 (przed kosztem i użytecznością).
  - H2: poparcie/endorsement KIDP/KRDP lub równoważnej instytucji branżowej istotnie przyspiesza adopcję.
  - H3: demonstracja trafności wraz z cytowaniami i śladem źródłowym buduje zaufanie szybciej niż referencje i testimonials.
- Kontrargumenty konkurencyjne i alternatywne wyjaśnienia:
  - doradcy mogą obawiać się przede wszystkim poufności/tajemnicy zawodowej, a nie odpowiedzialności.
  - kluczową barierą może być brak czasu na naukę, integracja z praktyką kancelarii i narzędziami, a nie zaufanie.
  - endorsement instytucji może być mniej istotny niż „peer proof” od wiodących kancelarii lub wymogi klientów korporacyjnych.
- Ramy analityczne:
  - Jobs-to-be-Done dla doradców podatkowych (co próbują osiągnąć, gdzie AI pomaga, gdzie przeszkadza).
  - Modele adopcji technologii: TAM/UTAUT, Diffusion of Innovations.
  - Zarządzanie zmianą: ADKAR lub równoważne.
  - Analiza ryzyka: ryzyko merytoryczne, prawne, reputacyjne, operacyjne, bezpieczeństwa informacji.

3. PARAMETRY RYNKU I OPERACJI
- Rynek docelowy: licencjonowani doradcy podatkowi w Polsce oraz kancelarie doradztwa podatkowego.
- Segmentacja do zastosowania w analizie (zawsze rozdzielaj wnioski, jeśli źródła na to pozwalają):
  - praktyka jednoosobowa,
  - mała kancelaria (2–10),
  - średnia/większa kancelaria,
  - specjalizacje: VAT, CIT, PIT, ceny transferowe, MDR, spory, KSeF.
- Krajobraz alternatyw i substytutów: komercyjne systemy informacji prawnej, oprogramowanie podatkowo-księgowe, manualny research w źródłach MF/ISAP/orzecznictwo, ogólne LLM, szkolenia/biuletyny.
- Horyzont czasowy: strategia adopcji i pilota 60–90 dni, z perspektywą skalowania w 6–12 miesięcy.
- Otoczenie regulacyjne i ograniczenia: tajemnica zawodowa i standardy zawodowe doradców podatkowych, RODO i bezpieczeństwo informacji, odpowiedzialność cywilna/kontraktowa, zasady dokumentowania pracy.
- Wymagane dane i typy dowodów:
  - badania ankietowe i raporty dotyczące AI w zawodach regulowanych w Polsce (prawnicy, księgowi, doradcy podatkowi),
  - stanowiska i publikacje organizacji branżowych,
  - artykuły naukowe o zaufaniu do AI w środowiskach profesjonalnych,
  - case studies wdrożeń w kancelariach prawnych/księgowych (Polska lub UE),
  - dane o incydentach, wyciekach, karach i postępowaniach dyscyplinarnych związanych z narzędziami cyfrowymi (jeśli dostępne).
- Etyka i reputacja: unikaj wniosków sugerujących „automatyzację odpowiedzialności” lub zastąpienie profesjonalnego osądu; uwzględnij ryzyko reputacyjne w razie błędu AI.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: praktyczny playbook wdrożeniowy dla leadershipu produktu, z wyraźnym rozdziałem „dowody PL” vs „analogi”.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1: podsumowanie dla zarządu (1–2 strony)
  - [ ] Tier 2: analiza operacyjna (5–8 stron)
  - [X] Tier 3: kompleksowy business case z ryzykami, segmentacją i planem pilota
- Wymagane elementy:
  - [X] Customer Insights & Pain Points
  - [X] Risk Assessment & Mitigation Strategies
  - [X] Implementation Roadmap with Milestones
  - [X] Organizational Impact & Change Management
  - [ ] Market Size & Growth Projections (uwzględnij tylko jeśli znajdziesz solidne dane)
  - [ ] Financial Model & ROI Projections (tylko jeśli masz dane do sensownych założeń; w przeciwnym razie opisz, jak je zebrać)
  - [X] [Other: matryca barier i artefaktów dowodowych]
- Wymagania dot. wizualizacji: jedna tabela „Barrier-to-Mitigation” oraz tabelaryczny plan pilota.
- Odbiorcy: product leadership, compliance lead, head of growth/marketing, customer success.
- Język: całość po polsku.

Wymagany format wyjścia (po polsku, zachowaj kolejność i strukturę):
1) Barrier Summary: 8–12 punktów, każdy punkt z przypisem/cytowaniem do źródła (link, data, instytucja). Jeśli twierdzenie jest wnioskowaniem, oznacz „wniosek syntetyczny” i podeprzyj go co najmniej dwoma źródłami.
2) Tabela Barrier-to-Mitigation: Bariera | Dowód (PL/UE/Global + cytaty) | Nasilenie (1–5) | Mitigacja | Artefakt dowodowy (co pokazujemy użytkownikowi).
3) Plan pilota 60–90 dni: cele, metryki sukcesu, kontrola zgodności, onboarding, pętla feedbacku, kryteria „stop/go”, komunikacja ryzyk i eskalacje.
4) Ocena pewności: dla każdej głównej bariery wskaż pewność (wysoka/średnia/niska), kluczowe niewiadome oraz lista pytań do wywiadów (12–20 pytań) z doradcami i partnerami kancelarii.

Zasady cytowania i weryfikacji w raporcie:
- Każde twierdzenie o „częstości”, „skali”, „najczęściej”, „nr 1” wymaga cytowanego źródła. Jeśli brak danych, napisz wprost „brak danych ilościowych w Polsce” i nie zastępuj tego liczbami.
- Rozdzielaj: dowód ilościowy, dowód jakościowy, opinia ekspercka, materiał marketingowy.
- Jeśli źródła są sprzeczne, pokaż oba i wyjaśnij rozbieżność.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet źródeł:
  - Tier 1 (najwyższy): polskie źródła instytucjonalne i branżowe (organizacje samorządowe, regulatorzy, instytucje publiczne), recenzowane publikacje naukowe, raporty z jasną metodologią, wyniki badań ankietowych z ujawnioną próbą.
  - Tier 2: raporty UE, badania międzynarodowe dot. zawodów regulowanych, analizy firm badawczych i konsultingowych z metodologią.
  - Tier 3: prasa branżowa, blogi eksperckie, studia przypadków vendorów (tylko jako kontekst, z oznaczeniem ryzyka stronniczości).
- Wykluczenia: anonimowe fora, materiały bez metodologii, twierdzenia bez źródeł, dane przestarzałe (zwyczajowo starsze niż 3–5 lat, chyba że w Polsce brak nowszych).
- Wnioski z innych branż: prawnicy, audyt, księgowość, medycyna jako analogie wdrożeń w profesjach o wysokiej odpowiedzialności; oznacz analogiczność i ograniczenia.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Ocena siły dowodu: dla każdej bariery oznacz „mocne/umiarkowane/słabe” i wyjaśnij na podstawie typu źródła i metodologii.
- Ograniczanie biasów: aktywnie szukaj dowodów obalających hipotezy H1–H3; uwzględnij perspektywę sceptyków, nie tylko entuzjastów.
- Scenariusze adopcji: best/base/worst case z uzasadnieniem oraz warunkami brzegowymi (co musi się wydarzyć, aby scenariusz zaszedł).
- Analiza wrażliwości: pokaż, które elementy (endorsement, mechanizmy audytu, polityka poufności, integracje) najbardziej zmieniają decyzję adopcyjną.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Projekt pilota: zaproponuj konstrukcję pilota dla segmentu sceptycznych profesjonalistów, minimalizując ryzyko poufności i odpowiedzialności.
- KPI i metryki: adopcja, aktywność, czas do pierwszej wartości, wskaźniki zaufania, liczba eskalacji do człowieka, błędy wykryte przed wysyłką do klienta, wskaźniki zgodności.
- Pętle feedbacku: kanały, częstotliwość, format; jak zamieniać feedback na backlog produktu i zmiany polityk.
- Protokół iteracji: rytm przeglądów (co tydzień w pilocie, potem co miesiąc), kryteria rozszerzania funkcji i segmentów.


PROMPT 2 (Q14) — Zadania najbardziej podatne na błędy i wzorce prewencji (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy analityk zgodności podatkowej, specjalista analizy błędów (quality assurance) w doradztwie i księgowości, ekspert ds. kontroli podatkowych i wyników audytów, konsultant responsible AI dla zastosowań regulowanych.
- Domena biznesowa: identyfikacja i redukcja błędów w praktyce doradztwa podatkowego w Polsce, projektowanie bezpiecznych modułów „mistake prevention” dla Polish Tax Advisor GPT.
- Cel strategiczny: wskazać najbardziej error-prone zadania w polskiej praktyce podatkowej, opisać typowe kategorie pomyłek i konsekwencje, a następnie zaproponować bezpieczne wzorce wsparcia AI oraz obowiązkowe „human review triggers”.
- Aktualny stan wiedzy: istnieją rozproszone dane z kontroli, raportów organów, NIK, publikacji MF/KAS, orzecznictwa i komentarzy; potrzebna synteza z rozróżnieniem częstotliwości i dotkliwości.
- Luki i szansa: brak ustrukturyzowanej mapy „gdzie i dlaczego doradcy/firmy popełniają błędy” oraz jak narzędzia AI mogą zmniejszyć ryzyko bez wchodzenia w rolę udzielania porad zamiast profesjonalisty.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Źródła po tej dacie oznacz jako „po cutoff” i nie używaj do kluczowych tez.
- Preferuj dane polskie; jeśli brakuje ilościowych, użyj analogów UE/świat z wyraźnym oznaczeniem.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie strategiczne: które zadania doradcze i compliance w Polsce są najbardziej podatne na błędy oraz jak asystent AI może pomagać w ich prewencji w sposób bezpieczny i zgodny?
- Hipotezy do weryfikacji:
  - H1: VAT ma najwyższą częstość błędów spośród kategorii podatkowych.
  - H2: błędy proceduralne i terminowe występują częściej niż błędy merytorycznej interpretacji.
  - H3: wsparcie AI oparte o checklisty może bezpiecznie redukować błędy proceduralne.
- Kontrargumenty i alternatywne wyjaśnienia:
  - częstotliwość błędów może wynikać z intensywności kontroli w danym obszarze, a nie z faktycznej „podatności”.
  - „najbardziej kosztowne” błędy mogą nie być „najczęstsze”.
- Ramy analityczne:
  - Matryca ryzyka: częstotliwość vs dotkliwość konsekwencji.
  - Klasyfikacja błędów: obliczeniowe, interpretacyjne, proceduralne, terminowe, dowodowe/dokumentacyjne, komunikacyjne.
  - Wzorce prewencji AI: checklisty, walidacje, wykrywanie braków, alerty terminów, weryfikacja źródeł, porównania z regułami, generowanie pakietu roboczego.

3. PARAMETRY RYNKU I OPERACJI
- Zakres praktyki: doradztwo podatkowe w Polsce, w tym rozliczenia VAT, CIT, PIT, JPK, raportowania, wnioski, interpretacje, korespondencja z organami, przygotowanie dokumentów i analiz.
- Źródła danych dla „error rates” i wzorców błędów:
  - raporty organów podatkowych i kontroli, sprawozdania instytucji, raporty NIK, publikacje MF/KAS, komunikaty i statystyki, jeśli dostępne publicznie,
  - omówienia typowych błędów w opracowaniach branżowych o ujawnionej metodologii,
  - orzecznictwo i interpretacje wskazujące powtarzalne obszary sporów i korekt,
  - badania/raporty dotyczące jakości w księgowości i compliance (Polska lub analog UE).
- Ograniczenia: brak publicznych, jednoznacznych statystyk dla części obszarów; w takim przypadku należy przejść na „dowody pośrednie” i jasno oznaczyć.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: raport produktowo-zgodnościowy z rekomendacjami modułów prewencji.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1: podsumowanie (1–2 strony)
  - [ ] Tier 2: analiza operacyjna (5–8 stron)
  - [X] Tier 3: kompleksowa analiza z matrycą ryzyka i rekomendacjami modułów MVP
- Wymagane elementy:
  - [X] Customer Insights & Pain Points (błędy i ich przyczyny w praktyce)
  - [X] Risk Assessment & Mitigation Strategies (jak AI zmniejsza ryzyko i gdzie może je zwiększać)
  - [X] Implementation Roadmap with Milestones (moduły MVP i walidacja)
  - [X] [Other: taxonomy błędów + human review triggers]
- Język: po polsku.

Wymagany format wyjścia:
1) Lista zadań najbardziej podatnych na błędy: Top 10 z krótkim opisem, konsekwencjami i cytowaniami (każda pozycja musi mieć dowód; jeśli nie ma ilościowego, dopuszczalne jest jakościowe z jasnym oznaczeniem).
2) Tabela taksonomii błędów: Zadanie | Najczęstsze błędy | Konsekwencja (kary, odsetki, korekty, ryzyko sporu, reputacja) | Dowód (źródło) | Wzorzec prewencji GPT | Wyzwalacz obowiązkowej weryfikacji przez człowieka.
3) Propozycje modułów „Error-Prevention” (3–6) dla MVP: opis funkcji, wymagane dane, ograniczenia, ryzyka, jak mierzyć efekt (KPI).
4) Ocena pewności: co jest dobrze udokumentowane, gdzie brak danych, jakich danych/wywiadów potrzeba.

Zasady dowodowe:
- Twierdzenia o częstości, „najczęstsze”, „najwięcej” wymagają cytatu. Nie twórz liczb ani rankingów bez źródeł.
- Przy każdym wzorcu prewencji oznacz: „wymaga walidacji” jeśli dotyczy merytorycznej interpretacji lub obszarów o wysokiej niepewności.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet źródeł:
  - Tier 1: polskie źródła publiczne o charakterze instytucjonalnym i raportowym, orzecznictwo i interpretacje jako dowód „powtarzalności problemu”, dokumenty z jawną metodyką.
  - Tier 2: publikacje naukowe, raporty branżowe z metodyką, analogi UE.
  - Tier 3: artykuły branżowe i komentarze ekspertów (tylko jako kontekst).
- Wykluczenia: materiały bez autorstwa/metodyki, sensacyjne wpisy, dane niezweryfikowane.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Triangulacja: dla topowych zadań szukaj potwierdzeń w co najmniej dwóch niezależnych źródłach.
- Rozdziel: „częstość” od „dotkliwości” i pokaż oba wnioski.
- Wskaż ryzyka wprowadzone przez AI: fałszywa pewność, błędne cytowania, nieaktualność prawa, halucynacje; zaproponuj kontrolki bezpieczeństwa.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Pilot walidacyjny modułów: jak testować redukcję błędów (próbki spraw, checklista jakości, audyt wewnętrzny).
- KPI: liczba wykrytych braków przed wysyłką, spadek korekt, czas pracy, satysfakcja i zaufanie.
- Pętla feedbacku: mechanizm zgłaszania błędów i uczenia bazy wiedzy, bez wprowadzania wrażliwych danych klienta poza bezpieczne kanały.


PROMPT 3 (Q16) — Potrzeby dostępu do informacji i architektura KB + Actions (Tier 5: Refinement)

1. KONTEKST TECHNICZNY I RAMY
- Eksperci prowadzący badanie: starszy architekt informacji (systemy wiedzy prawnej), inżynier systemów RAG/knowledge retrieval, specjalista ds. prywatności i bezpieczeństwa (RODO, tajemnica zawodowa), praktyk workflow doradztwa podatkowego.
- Domena techniczna: architektura źródeł wiedzy i dostępu do informacji dla Polish Tax Advisor GPT (baza wiedzy, wyszukiwanie, cytowania, Actions/integracje), z naciskiem na zgodność i minimalizację danych.
- Cel badania: zmapować typowe potrzeby informacyjne polskiego doradcy podatkowego oraz zaproponować architekturę „knowledge base + Actions”, która maksymalizuje użyteczność i zgodność (privacy-by-design, auditability-by-design).
- Aktualny stan zrozumienia: istnieją liczne źródła prawa i praktyki (ustawy, rozporządzenia, objaśnienia, interpretacje, orzecznictwo), a także zasoby komercyjne; brak jednolitego modelu ich klasyfikacji i sposobu bezpiecznego dostępu przez GPT.
- Luki i ryzyko: ryzyko nieaktualności, ryzyko niewłaściwego cytowania, ryzyko naruszenia poufności przy danych klienta i zasobach firmowych.
- Potencjalny wpływ: decyzje o tym, co wbudować w KB, co pobierać na żądanie, jakie Actions są konieczne w MVP, oraz jakie kontrolki prywatności wdrożyć.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Nie opieraj tez na źródłach po tej dacie; oznacz je jako „po cutoff”.
- Preferuj źródła polskie; dla prawa UE i orzecznictwa unijnego używaj źródeł oficjalnych UE.
- Język raportu: polski.

2. KLUCZOWE PYTANIE TECHNICZNE I HIPOTEZY
- Główne pytanie techniczne: jakich typów informacji potrzebuje polski doradca podatkowy i jaka architektura „KB + Actions” zapewni dostęp w sposób zgodny i praktyczny?
- Hipotezy do weryfikacji:
  - H1: baza interpretacji/wyjaśnień organów podatkowych (MF/KIS lub równoważne źródła instytucjonalne) jest jednym z najczęściej używanych zasobów zewnętrznych.
  - H2: dane klienta wymagają integracji Actions (pobieranie/operowanie na danych na żądanie), a nie wbudowania w wiedzę modelu.
  - H3: szablony i playbooki firmowe są bardzo wartościowe, ale trudne do integracji w sposób zgodny (klasyfikacja, uprawnienia, retencja, ślad audytowy).
- Alternatywne rozwiązania: pełny model „wyszukiwarka + cytowania bez KB”, rozwiązania hybrydowe (KB tylko dla stabilnego prawa, reszta on-demand), integracja z komercyjnymi systemami informacji prawnej.
- Podstawa teoretyczna: zasady privacy-by-design i data minimization, architektura RAG z cytowaniami i kontrolą wersji, zasady bezpieczeństwa informacji.

3. PARAMETRY METODOLOGICZNE
- Podejście: inwentaryzacja i klasyfikacja źródeł oraz potrzeb informacyjnych, mapowanie workflow doradcy podatkowego, ocena ryzyk prywatności i bezpieczeństwa dla każdej kategorii danych, ocena częstotliwości aktualizacji i dostępności technicznej (API, RSS, pobieranie stron).
- Typy dowodów: dokumenty instytucjonalne, opisy systemów źródłowych, regulaminy dostępu, publikacje branżowe o workflow, raporty o praktykach informacyjnych w zawodach prawniczych/podatkowych.
- Zakres czasowy: źródła do 17 stycznia 2026.
- Zakres systemów: zewnętrzne źródła prawa i praktyki, dane klienta, zasoby kancelarii, integracje narzędzi biurowych.

4. SPECYFIKACJA WYJŚCIA
- Struktura raportu: specyfikacja architektury informacji dla produktu z tabelami, klasyfikacją danych i rekomendacjami MVP.
- Streszczenie wymagane: Tak.
- Głębokość analizy:
  - [ ] Level 1: executive memo
  - [ ] Level 2: pełny przegląd (z diagramami)
  - [X] Level 3: kompleksowy raport z ryzykami, kontrolami i listą Actions (MVP + roadmap)
- Wymagane elementy:
  - [X] Architecture Overview
  - [X] Risk Matrix (privacy/security)
  - [X] Remediation Roadmap (kontrole i etapy)
  - [X] [Other: mapa potrzeb informacyjnych i metody dostępu]
- Wymagania dot. wizualizacji: jedna tabela architektoniczna oraz schemat logiczny (tekstowy) KB vs Actions.

Wymagany format wyjścia (po polsku):
1) Mapa potrzeb informacyjnych: wypunktowanie wg kategorii (prawo „statyczne”, praktyka i wytyczne „dynamiczne”, orzecznictwo, dane klienta, zasoby firmowe).
2) Tabela architektury: Kategoria informacji | Typowe źródła | Częstotliwość zmian | Metoda dostępu (KB, wyszukiwanie on-demand, Action/API, upload dokumentu) | Ryzyko prywatności (niske/średnie/wysokie) | Kontrola i zabezpieczenia (uprawnienia, logowanie, retencja, anonimizacja).
3) Rekomendowana architektura: proponowana struktura KB (taksonomia, wersjonowanie, zasady aktualizacji, cytowania) oraz lista Actions dla MVP (z krótkim opisem celu, danych wejściowych, minimalizacji danych, autoryzacji).
4) Ocena pewności: co opiera się na twardych źródłach, a co jest „estymacją praktyka”; lista pytań do wywiadów z doradcami i administratorami systemów w kancelariach.

Zasady weryfikacji:
- Twierdzenia o „najczęściej używanych źródłach” cytuj lub oznacz jako „estymacja praktyka” z uzasadnieniem.
- Dla każdej kategorii danych wymień ryzyka i kontrolki; jeśli nie jesteś pewien, oznacz „wymaga walidacji prawnej/bezpieczeństwa”.

5. HIERARCHIA DOWODÓW I JAKOŚĆ ŹRÓDEŁ
- Priorytet:
  - Tier 1: oficjalne źródła prawa (publikatory), oficjalne serwisy instytucji, oficjalne bazy interpretacji/objaśnień, oficjalne rejestry i portale; dokumentacja techniczna systemów i mechanizmów dostępu.
  - Tier 2: uznane systemy informacji prawnej i opracowania z metodyką, publikacje naukowe o systemach wiedzy prawnej.
  - Tier 3: komentarze branżowe i blogi (kontekst).
- Wykluczenia: materiały bez wskazania źródeł, treści przestarzałe bez oznaczeń.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Oznaczaj źródła jako „wiążące” vs „pomocnicze” dla praktyki doradcy.
- Sprawdzaj aktualność i wersję dokumentu; opisuj mechanizm „jak uniknąć nieaktualności” w architekturze.
- Dokumentuj niepewności i założenia (data minimization, retencja, dostęp).

7. PROTOKÓŁ WALIDACJI
- Analiza rozwiązań alternatywnych: co najmniej dwa warianty architektury (konserwatywny vs ambitny), z plusami/minusami.
- Symulacja review: zasugeruj checklistę przeglądu przez IOD/bezpieczeństwo oraz przez praktyka doradztwa podatkowego.
- Strategia reewaluacji: jak często przeglądać listę źródeł i Actions wraz ze zmianami prawa i narzędzi.


PROMPT 4 (Q18) — Ocena dojrzałości cyfrowej i onboarding (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy strateg customer success dla usług profesjonalnych, specjalista ds. dojrzałości cyfrowej, badacz adopcji technologii w Polsce, projektant onboardingu dla narzędzi regulowanych.
- Domena biznesowa: poziom digitalizacji pracy doradców podatkowych w Polsce oraz projektowanie infrastruktury wdrożeniowej i szkoleniowej dla Polish Tax Advisor GPT.
- Cel strategiczny: określić dojrzałość cyfrową segmentów doradców podatkowych, kluczowe narzędzia i procesy, oraz zaprojektować onboarding/training tracks minimalizujące barierę startu i ryzyko błędów.
- Aktualny stan wiedzy: istnieją wskaźniki digitalizacji administracji podatkowej (e-usługi, e-filing, JPK, KSeF), ale trzeba je przełożyć na dojrzałość praktyk i kancelarii.
- Decyzje, które zależą od wyników: treści szkoleniowe, format onboarding (szablony promptów, tutoriale), wymagania techniczne, segmentacja marketingowa, priorytety funkcji.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Źródła po tej dacie oznacz jako „po cutoff”.
- Preferuj dowody polskie; analogi UE/świat tylko gdy brak PL.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie strategiczne: jaki jest poziom dojrzałości cyfrowej polskich doradców podatkowych i jaka infrastruktura onboardingowo-szkoleniowa jest potrzebna, aby skutecznie wdrożyć Polish Tax Advisor GPT?
- Hipotezy do weryfikacji:
  - H1: adopcja e-rozliczeń i e-usług jest wysoka, co sugeruje bazowy komfort cyfrowy.
  - H2: młodsi doradcy mają wyższy komfort korzystania z AI/automatyzacji.
  - H3: onboarding oparty o gotowe szablony i scenariusze (biblioteka promptów i przykładów zastosowań) jest najskuteczniejszy.
- Kontrargumenty:
  - wysoka adopcja e-usług nie oznacza umiejętności pracy z narzędziami AI.
  - dojrzałość może zależeć bardziej od wielkości kancelarii i typu klientów niż od wieku.
- Ramy analityczne: model dojrzałości cyfrowej (poziomy 1–5), segmentacja person, analiza „workflow readiness”.

3. PARAMETRY RYNKU I OPERACJI
- Rynek docelowy: doradcy podatkowi w Polsce (samodzielni i kancelarie).
- Obszary do zbadania:
  - aktualnie używane narzędzia: systemy księgowe, e-usługi administracji, systemy informacji prawnej, pakiety biurowe, narzędzia do obiegu dokumentów, CRM, podpis elektroniczny, e-doręczenia.
  - wskaźniki infrastruktury: wykorzystanie e-usług podatkowych, JPK, KSeF i innych obowiązków cyfrowych, jeśli dostępne publicznie.
  - kompetencje: automatyzacja, praca na szablonach, bezpieczeństwo informacji, praca na źródłach.
- Dane wymagane: statystyki adopcji (jeśli istnieją), raporty branżowe, badania kompetencji cyfrowych, publikacje instytucji, dane o wdrożeniach KSeF/JPK w firmach, jeśli dostępne.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: raport onboardingowy z segmentacją i rekomendacjami ścieżek wdrożenia.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1
  - [ ] Tier 2
  - [X] Tier 3
- Wymagane elementy:
  - [X] Customer Insights & Pain Points
  - [X] Risk Assessment & Mitigation Strategies (ryzyko błędów, bezpieczeństwo, prywatność)
  - [X] Implementation Roadmap with Milestones (onboarding)
  - [X] Organizational Impact & Change Management
  - [X] [Other: wskaźniki dojrzałości + artefakty wsparcia]
- Język: polski.

Wymagany format wyjścia:
1) Podsumowanie dojrzałości cyfrowej: 8–10 punktów, z cytowaniami przy danych/statystykach i rozróżnieniem segmentów.
2) Tabela wskaźników dojrzałości: Wskaźnik | Dowód (źródło, rok, metodyka) | Implikacja dla wdrożenia | Artefakt wsparcia (co dostarczamy: tutorial, checklist, integracja).
3) Plan onboardingu: 2–4 ścieżki wg persony (rola, dojrzałość, typ kancelarii), z modułami szkoleniowymi, czasem trwania, ćwiczeniami, zasadami bezpieczeństwa i miernikami postępu.
4) Ocena pewności: obszary wymagające walidacji, brakujące dane, propozycja badań (ankieta/wywiady) i pytania.

Zasady dowodowe:
- Statystyki adopcji muszą mieć cytat do źródła (instytucja, data, link). Jeśli nie znajdziesz statystyk, nie twórz ich.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet:
  - Tier 1: dane instytucji publicznych i raporty oficjalne, badania z metodyką, publikacje organizacji branżowych.
  - Tier 2: raporty rynkowe i akademickie, analogi UE.
  - Tier 3: artykuły branżowe (kontekst).
- Wykluczenia: ogólne opinie bez metodyki, dane nieopisane.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Triangulacja: porównuj co najmniej dwa niezależne źródła dla kluczowych wniosków o dojrzałości.
- Segmentacja: jeśli dane nie pozwalają na segmentację, oznacz ograniczenie i zaproponuj jak ją zebrać.
- Scenariusze: wskaż ryzyka przy onboardingu (błędy, poufność, fałszywe zaufanie do AI) i kontrolki.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Pilot onboardingowy: jak przetestować ścieżki na małej grupie doradców.
- KPI: ukończenie onboardingu, aktywacja, pierwsza wartość, zaufanie, liczba eskalacji do człowieka, incydenty bezpieczeństwa.
- Feedback loop: cykliczne retrospektywy i aktualizacje materiałów szkoleniowych w oparciu o pytania użytkowników.


PROMPT 5 (Q19) — Progi opłacalności i pozycjonowanie cenowe (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy analityk propozycji wartości (B2B), specjalista ROI w usługach profesjonalnych, analityk rynku SaaS w Polsce, ekspert ds. ekonomiki kancelarii podatkowych.
- Domena biznesowa: wycena i uzasadnienie ROI dla Polish Tax Advisor GPT w segmencie doradców podatkowych w Polsce; projektowanie pozycjonowania cenowego i argumentów wartości.
- Cel strategiczny: ustalić progi oszczędności czasu/kosztów i oczekiwania zwrotu, które uzasadniają zakup; przygotować model ROI dla segmentów i rekomendacje pricing/packaging.
- Aktualny stan wiedzy: istnieją ogólne wzorce ROI w SaaS, ale potrzebna adaptacja do ekonomiki doradztwa podatkowego w Polsce i realiów kancelarii.
- Decyzje zależne: ceny MVP, pakiety, komunikacja ROI, proof points, program pilota wartości.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Źródła po tej dacie oznacz i nie używaj do kluczowych tez.
- Preferuj dane polskie (PLN). Jeśli używasz EUR lub innych walut, podaj kurs i źródło kursu oraz datę.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie: jakie progi oszczędności czasu/kosztów uzasadniają adopcję Polish Tax Advisor GPT u polskich doradców podatkowych i jak pozycjonować cenę?
- Hipotezy do weryfikacji:
  - H1: standardem oczekiwań jest payback 3–6 miesięcy.
  - H2: oszczędność 2+ godzin tygodniowo uzasadnia abonament 50–100 EUR miesięcznie.
  - H3: pozycjonowanie oparte o wartość (czas, ryzyko, jakość) działa lepiej niż pozycjonowanie „taniej”.
- Kontrargumenty:
  - próg może zależeć od liczby spraw o wysokiej wartości i od ryzyka odpowiedzialności, nie od czasu.
  - doradcy mogą preferować rozliczenie za użytkownika, za kancelarię lub za wolumen, nie stały abonament.
- Ramy analityczne: value-based pricing, unit economics kancelarii, payback i analiza wrażliwości, segmentacja person.

3. PARAMETRY RYNKU I OPERACJI
- Segmenty/persony do modelu ROI (minimum 3):
  - doradca solo,
  - mała kancelaria,
  - średnia/większa kancelaria.
- Dane wymagane:
  - stawki godzinowe lub ekwiwalent wartości godziny pracy (z badań, raportów, cenników publicznych, jeśli rzetelne),
  - liczba godzin poświęcana na research, przygotowanie pism, weryfikacje, monitoring zmian,
  - koszty alternatyw (systemy informacji prawnej, szkolenia, asystenci),
  - oczekiwania zakupowe B2B: payback, budżety narzędziowe, akceptowane ceny w Polsce.
- Ograniczenia: jeśli dane o stawkach są rozproszone i niepewne, pokaż zakresy i źródła oraz wykonaj analizę wrażliwości.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: model ROI + rekomendacje pricing/positioning + proof points.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1
  - [ ] Tier 2
  - [X] Tier 3
- Wymagane elementy:
  - [X] Financial Model & ROI Projections (z jawnymi założeniami)
  - [X] Risk Assessment & Mitigation Strategies (ryzyko błędów, reputacji, compliance jako część wartości)
  - [X] Implementation Roadmap with Milestones (jak zebrać proof points)
  - [X] [Other: rekomendacje pricing/packaging i „jak to sprzedać zgodnie”]
- Język: polski.

Wymagany format wyjścia:
1) Podsumowanie progów ROI: kluczowe wnioski (w punktach), z cytowaniami dla oczekiwań rynkowych, a dla obliczeń z jawnymi założeniami.
2) Tabela modelu ROI: Persona | Ekonomika bazowa (założenia: stawka, liczba godzin, koszty narzędzi) | Wymagana oszczędność czasu | Maksymalna cena miesięczna | Dowód (źródła dla stawek/benchmarków) | Wrażliwość (co jeśli założenia zmienią się o ±20%).
3) Rekomendacje pozycjonowania ceny: warianty pakietów, zakotwiczenia wartości, „proof points” (jak mierzyć oszczędność i ryzyko) oraz komunikaty zgodne (bez obietnic nieomylności).
4) Ocena pewności: które założenia mają twarde dane, a które wymagają badań; lista pytań do wywiadów i propozycja krótkiej ankiety.

Zasady obliczeń i dowodów:
- Każda kalkulacja ma pokazywać: dane wejściowe, jednostki, źródło, wzór i wynik.
- Nie twórz stawek ani danych kosztowych bez źródła; jeśli brak, pokaż zakres i opisz, jak go pozyskać.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet:
  - Tier 1: polskie raporty i badania z metodyką, dane instytucjonalne, wiarygodne benchmarki B2B, publiczne dane rynkowe.
  - Tier 2: analizy UE/świat dot. ROI w usługach profesjonalnych, raporty rynkowe z metodologią.
  - Tier 3: prasa i blogi (kontekst).
- Wykluczenia: „pricing pages” bez kontekstu, tezy bez metodologii.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Analiza wrażliwości: przetestuj wpływ kluczowych założeń na wynik (czas oszczędzony, wykorzystanie, stawka).
- Bias: nie zakładaj automatycznie, że oszczędność czasu = równoważna wartości pieniężnej; pokaż ograniczenia.
- Scenariusze: base/best/worst, plus warunki.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Pilot pomiaru ROI: jak mierzyć realne oszczędności i ryzyka w pilocie (time tracking, ankiety, audyt jakości).
- KPI: oszczędność czasu, spadek korekt, skrócenie cyklu przygotowania dokumentu, wzrost satysfakcji, spadek stresu compliance.
- Iteracja pricing: jak aktualizować cennik i pakiety po zebraniu dowodów.


PROMPT 6 (Q21) — Kadencja aktualizacji wiedzy i workflow utrzymania (Tier 5: Refinement)

1. KONTEKST TECHNICZNY I RAMY
- Eksperci prowadzący badanie: senior content operations manager (prawo/regulacje), specjalista monitoringu zmian legislacyjnych, architekt wersjonowania treści i kontroli jakości, praktyk compliance w produktach AI.
- Domena techniczna: monitoring zmian prawa podatkowego i praktyki organów w Polsce, projektowanie procesu aktualizacji wiedzy i SLA dla Polish Tax Advisor GPT.
- Cel badania: określić, jak często zmieniają się kluczowe źródła i jakie zdarzenia wymagają pilnych aktualizacji; zaproponować realistyczną kadencję aktualizacji i workflow utrzymania.
- Aktualny stan zrozumienia: mnogość źródeł (ustawy/rozporządzenia, objaśnienia, interpretacje, orzecznictwo, komunikaty), różna częstotliwość i waga zmian; potrzebna systematyka.
- Wpływ: decyzje o budżecie utrzymania, narzędziach monitoringu, strukturze bazy wiedzy, testach regresji i mechanizmach „freshness”.

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz „po cutoff”.
- Priorytet: Polska; w zakresie prawa UE i orzecznictwa używaj źródeł oficjalnych.

2. KLUCZOWE PYTANIE TECHNICZNE I HIPOTEZY
- Główne pytanie: jak często zmieniają się krytyczne wytyczne i praktyka podatkowa w Polsce oraz jaka kadencja utrzymania zapewni akceptowalną aktualność i minimalne ryzyko błędów w tax GPT?
- Hipotezy do weryfikacji:
  - H1: interpretacje i bieżące komunikaty wymagają monitoringu co najmniej tygodniowego; zmiany ustawowe można przeglądać w cyklu miesięcznym.
  - H2: dla większości krytycznych źródeł istnieją kanały monitoringu (RSS, newsletter, BIP, API, feed publikatora) możliwe do automatyzacji.
  - H3: kwartalny przegląd całościowy jest wystarczający dla większości treści, przy osobnym trybie „urgent” dla zdarzeń krytycznych.
- Alternatywne ujęcia: tematy o wysokiej zmienności (VAT/KSeF) mogą wymagać częstszych przeglądów; orzecznictwo może wpływać na interpretacje bez zmian ustawowych.

3. PARAMETRY METODOLOGICZNE
- Podejście: ilościowe i jakościowe mapowanie częstotliwości publikacji i typów zmian dla każdej klasy źródeł; identyfikacja zdarzeń wyzwalających „urgent update”; przegląd narzędzi monitoringu i dobrych praktyk wersjonowania.
- Dane i dowody: statystyki publikacji (jeśli dostępne), archiwa komunikatów, dokumentacja kanałów monitoringu, opisy procesów w organizacjach prawniczych/compliance.
- Zakres: kluczowe obszary podatkowe i compliance; źródła instytucjonalne, orzecznictwo, praktyka.

4. SPECYFIKACJA WYJŚCIA
- Struktura raportu: rekomendacja kadencji + tabela triggerów + workflow utrzymania.
- Streszczenie wymagane: Tak.
- Głębokość analizy:
  - [ ] Level 1
  - [ ] Level 2
  - [X] Level 3
- Wymagane elementy:
  - [X] Architecture Overview (jak monitorujemy i aktualizujemy)
  - [X] Risk Matrix (ryzyko nieaktualności vs wpływ)
  - [X] Remediation Roadmap (wdrożenie procesu)
  - [X] [Other: SLA, wersjonowanie, testy regresji]
- Język: polski.

Wymagany format wyjścia:
1) Rekomendowana kadencja aktualizacji: wg tematu i typu źródła (tabela lub lista), z cytowaniami do danych o częstotliwości publikacji tam, gdzie to możliwe.
2) Tabela triggerów: Zdarzenie | Źródło | Waga/Severity (1–5) | Działanie | SLA (czas reakcji) | Kontrola jakości.
3) Workflow utrzymania: role, odpowiedzialności, wersjonowanie, mechanizmy diff, testy regresji, zasady publikacji, procedura „rollback”.
4) Ocena pewności: gdzie są twarde dane, gdzie trzeba walidacji; lista braków i pytań do zespołu merytorycznego.

Wymogi weryfikacyjne:
- Twierdzenia o częstotliwości publikacji muszą mieć cytat lub wyraźne oznaczenie „brak danych, estymacja na podstawie próby” wraz z opisem próby.
- Zawsze weryfikuj dostępność mechanizmów monitoringu (RSS/API/newsletter) w źródłach.

5. HIERARCHIA DOWODÓW I JAKOŚĆ ŹRÓDEŁ
- Tier 1: oficjalne publikatory i serwisy instytucji (z metadanymi dat), oficjalne archiwa i bazy, dokumentacja techniczna kanałów informacyjnych.
- Tier 2: opracowania z metodyką, raporty analityczne o zmianach prawa.
- Tier 3: komentarze branżowe jako kontekst trendów.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Rejestrowanie wersji: wymagaj propozycji „jak identyfikować wersję prawa i wiedzy użytej do odpowiedzi”.
- Unikanie luk: opisz, jak wykrywać brak monitoringu dla kluczowych źródeł i jak to kompensować.
- Oznaczanie stanu: „aktualne na dzień …” i „wymaga weryfikacji” dla obszarów o wysokiej zmienności.

7. PROTOKÓŁ WALIDACJI
- Alternatywne rozwiązania: porównaj co najmniej dwa modele utrzymania (częste małe aktualizacje vs rzadkie duże).
- Peer review: zaproponuj review merytoryczne i compliance.
- Reewaluacja: zaproponuj cykl przeglądu procesu utrzymania (kwartalnie) i kryteria zmiany kadencji.


PROMPT 7 (Q22) — Priorytety integracji Actions/API (MVP vs roadmap) (Tier 5: Refinement)

1. KONTEKST TECHNICZNY I RAMY
- Eksperci prowadzący badanie: starszy architekt integracji (API), architekt bezpieczeństwa i prywatności, praktyk automatyzacji w kancelariach, analityk wartości vs wysiłek (value/effort).
- Domena techniczna: projektowanie i priorytetyzacja integracji (GPT Actions) dla Polish Tax Advisor GPT w środowisku kancelarii podatkowych w Polsce.
- Cel badania: określić, które integracje generują największą wartość (czas, ryzyko, jakość) przy akceptowalnej wykonalności i ryzyku, oraz zaproponować MVP vs roadmap.
- Aktualny stan: wiele potencjalnych integracji (źródła prawa, e-usługi, systemy księgowe, DMS, poczta, kalendarz); potrzebna macierz priorytetów i minimalna architektura bezpieczeństwa.
- Wpływ: backlog produktu, plan prac technicznych, wymagania partnerstw i zgodności.

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz.
- Preferuj informacje z dokumentacji API i źródeł instytucjonalnych lub vendorów.

2. KLUCZOWE PYTANIE TECHNICZNE I HIPOTEZY
- Główne pytanie: które integracje Actions dają największą wartość doradcom podatkowym w Polsce, a które powinny trafić do MVP vs roadmap?
- Hipotezy do weryfikacji:
  - H1: integracja z oficjalnym źródłem aktów prawnych (ISAP lub równoważne) daje najwyższą wartość w research i cytowaniach.
  - H2: integracje ERP/systemów księgowych są wysokowartościowe, ale wysokozłożone (API, wdrożenia, różnorodność).
  - H3: integracja KSeF powinna być priorytetem roadmapy, a nie MVP (potwierdź stan regulacyjny i gotowość ekosystemu na cutoff).
- Alternatywy: MVP bez integracji z systemami transakcyjnymi (tylko read-only i źródła publiczne), integracje przez eksport/import plików, integracje przez DMS.

3. PARAMETRY METODOLOGICZNE
- Podejście: identyfikacja listy typowych narzędzi używanych w kancelariach, przegląd dostępności API i warunków licencyjnych, analiza wymagań autoryzacji, ocena ryzyk prywatności i tajemnicy zawodowej, scoring value vs effort.
- Dane: dokumentacja API, warunki korzystania, przykłady wdrożeń, opisy integracji, wymagania prawne dot. danych.

4. SPECYFIKACJA WYJŚCIA
- Struktura raportu: lista top integracji + tabela priorytetyzacji + rekomendowana architektura MVP.
- Streszczenie wymagane: Tak.
- Głębokość analizy:
  - [ ] Level 1
  - [ ] Level 2
  - [X] Level 3
- Wymagane elementy:
  - [X] Architecture Overview
  - [X] Security Assessment
  - [X] Risk Matrix
  - [X] Remediation Roadmap
  - [X] [Other: tabela value/effort + wymagania vendor partnership]
- Język: polski.

Wymagany format wyjścia:
1) Top Actions Shortlist: krótka lista (z uzasadnieniem wartości i ograniczeń), z cytowaniami dot. dostępności API lub mechanizmów dostępu.
2) Tabela priorytetyzacji: Action | Wartość (1–5) + uzasadnienie | Wykonalność (1–5) + dowód z dokumentacji | Warunki wstępne | Ryzyko zgodności/prywatności (niske/średnie/wysokie) | Priorytet (MVP/Roadmap/Do not do) | Uwagi o partnerstwie.
3) Rekomendowana architektura MVP: minimalizacja danych, autoryzacja (preferuj OAuth/bezpieczne tokeny), logowanie i audyt, zakresy uprawnień, zasady przechowywania, izolacja tenantów, mechanizm wycofania zgody, obsługa incydentów.
4) Ocena pewności: które integracje mają jasną dokumentację i stabilne API, a które wymagają negocjacji lub walidacji; lista pytań do vendorów.

Wymogi weryfikacyjne:
- Wykonalność ma być podparta cytatem z dokumentacji API/warunków użycia.
- Oznacz integracje wymagające partnerstwa vendorowego, płatnych licencji lub ograniczeń prawnych.

5. HIERARCHIA DOWODÓW I JAKOŚĆ ŹRÓDEŁ
- Tier 1: oficjalna dokumentacja API, regulaminy, specyfikacje, oficjalne repozytoria, komunikaty instytucji.
- Tier 2: analizy techniczne i wdrożeniowe z metodologią, case studies.
- Tier 3: blogi i dyskusje techniczne jako kontekst, z ostrzeżeniem o wiarygodności.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Spójne kryteria scoringu: zdefiniuj, co oznacza 1–5 dla wartości i wykonalności.
- Minimalizacja danych: dla każdej integracji wskaż minimalny zestaw danych i ryzyko.
- Unikaj wniosków bez dowodów: jeśli API nie jest potwierdzone, oznacz „niepotwierdzone”.

7. PROTOKÓŁ WALIDACJI
- Porównaj co najmniej dwa warianty MVP: (A) tylko źródła publiczne + cytowania, (B) źródła publiczne + 1–2 integracje produktowo-kluczowe.
- Peer review: review bezpieczeństwa i review praktyka kancelarii.
- Reewaluacja: kwartalna rewizja priorytetów integracji wraz ze zmianami ekosystemu.


PROMPT 8 (Q24) — Mechanizmy weryfikacji zgodności i audytowalności (Tier 5: Refinement)

1. KONTEKST PRAWNY I RAMY
- Eksperci prowadzący badanie: prawnik/compliance officer w obszarze produktów AI, ekspert ds. standardów audytu i śladu dowodowego, specjalista ds. ochrony danych (RODO), praktyk doradztwa podatkowego (dokumentacja pracy, odpowiedzialność).
- Domena prawna: wymagania audytowalności i weryfikacji pracy wspieranej przez AI w doradztwie podatkowym w Polsce, w tym standardy dokumentacji, transparentność źródeł, kontrola wersji bazy wiedzy i logowanie.
- Cel badania: zidentyfikować mechanizmy weryfikacji i dowodzenia, które doradca podatkowy oraz kancelaria powinni mieć, aby bronić jakości i zgodności pracy wspieranej przez AI (z uwzględnieniem wymogów prawa, standardów i dobrych praktyk).
- Jurysdykcja: Polska + prawo UE mające zastosowanie (RODO, regulacje dotyczące AI i usług cyfrowych, jeśli relewantne).
- Aktualny stan wiedzy: istnieją ogólne standardy compliance (bezpieczeństwo, privacy, audit trails), ale trzeba je przełożyć na praktykę doradztwa podatkowego i defensibility outputów.
- Luki: niejednoznaczność wymogów co do logów, retencji, formy „work papers” dla interakcji z AI; potrzeba mapowania na źródła prawne i standardy.
- Potencjalny wpływ: decyzje produktowe (co logować, jak wersjonować, jak eksportować „pakiet audytowy”, polityki retencji i zgód).

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz „po cutoff”.
- Preferuj źródła oficjalne i wiążące; materiały vendorów tylko jako kontekst.

2. GŁÓWNE PYTANIE PRAWNE I HIPOTEZY
- Główne pytanie: jakie mechanizmy weryfikacji zgodności i audytowalności są niezbędne, aby audytować i bronić pracy doradcy podatkowego wspieranej przez AI?
- Hipotezy do weryfikacji:
  - H1: cytowanie konkretnych źródeł prawa i praktyki (z identyfikatorem wersji/daty) jest wystarczające dla defensibility audytowej w większości przypadków.
  - H2: doradcy potrzebują eksportowalnych „work papers” z interakcji z AI (pakiet audytowy).
  - H3: wersja bazy wiedzy użyta w momencie zapytania musi być logowana.
- Alternatywne interpretacje:
  - same cytowania mogą nie wystarczać bez kontekstu rozumowania i dokumentacji decyzji człowieka.
  - retencja i logowanie mogą podlegać ograniczeniom RODO i zasadom minimalizacji danych.

3. PARAMETRY METODOLOGICZNE
- Projekt badania: przegląd prawa i standardów (analiza normatywna), mapowanie wymogów na mechanizmy techniczne, analiza porównawcza dobrych praktyk w audycie (w tym w profesjach regulowanych).
- Materiały: ustawy i akty wykonawcze, dokumenty regulacyjne UE, wytyczne organów i samorządów zawodowych, standardy bezpieczeństwa/audytu (jeśli stosowane w takich produktach), publikacje eksperckie z jasno wskazaną podstawą.
- Metody analityczne: interpretacja przepisów, rozróżnienie „wymóg prawny” vs „dobra praktyka”, analiza ryzyk i zgodności z zasadami RODO.

4. SPECYFIKACJA WYJŚCIA PRAWNEGO
- Struktura raportu: memorandum compliance + matryca mechanizmów audytowych.
- Abstract: Nie (zastąp to streszczeniem menedżerskim).
- Głębokość analizy:
  - [ ] Level 1: krótki opis
  - [ ] Level 2: standardowe memorandum
  - [X] Level 3: kompleksowa opinia/analiza z mapowaniem na mechanizmy produktowe
- Wymagane elementy:
  - [X] Issue Statement
  - [X] Rule(s) of Law
  - [X] Application/Analysis (mapowanie na mechanizmy techniczne)
  - [X] Conclusion
  - [X] Recommendations (product requirements i polityki)
  - [X] [Other: podział „wymóg prawny” vs „best practice” + ryzyka]
- Styl cytowań: cytowania „plain” z linkami i datą; dla aktów prawnych podaj publikator/źródło oficjalne.

Wymagany format wyjścia (po polsku):
1) Podsumowanie mechanizmów weryfikacji: lista mechanizmów (punkty) z krótkim uzasadnieniem i cytatami do podstawy (prawo/standard/best practice).
2) Tabela audytowalności: Mechanizm | Co logować | Retencja (propozycja + ograniczenia) | Artefakt (co powstaje) | Ryzyko (co jeśli brak) | Podstawa (wymóg prawny / best practice + cytat).
3) Szablony outputów: opis zawartości „pakietu audytowego” dla jednej sprawy (co ma zawierać, jak chronić dane, jak eksportować).
4) Ocena pewności: elementy wymagające walidacji prawnej (lista) i pytania do prawnika/inspektora ochrony danych/samorządu zawodowego.

Wymogi dowodowe:
- Każdy „wymóg” musi mieć cytat do źródła prawnego/standardu albo zostać oznaczony jako „best practice” z uzasadnieniem i źródłem.
- Mechanizmy potencjalnie kolidujące z RODO lub tajemnicą zawodową oznacz jako „ryzyko – wymaga decyzji prawnej”.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Tier 1: akty prawne i oficjalne wytyczne, regulacje UE, dokumenty organów publicznych, dokumenty samorządu zawodowego.
- Tier 2: uznane opracowania i standardy branżowe (bezpieczeństwo, audyt, zarządzanie AI) z jasno określonym zakresem.
- Tier 3: materiały kontekstowe (artykuły branżowe) tylko pomocniczo.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Rozróżnij: co jest wiążące w Polsce, co jest perswazyjne lub rekomendowane.
- Wskazuj niepewności i obszary „unsettled”.
- Cytuj konkretny fragment/artykuł/sekcję, nie tylko stronę główną.

7. PROTOKÓŁ WALIDACJI
- Analiza alternatyw: co najmniej dwa warianty logowania/retencji (konserwatywny vs rozszerzony) z oceną ryzyk.
- Peer review: rekomenduj przegląd przez prawnika (podatki/RODO) oraz specjalistę bezpieczeństwa informacji.
- Strategia aktualizacji: zaproponuj, jak często weryfikować wymagania przy zmianach prawa i standardów.


PROMPT 9 (Q25) — Unikalna propozycja wartości (UVP) i pozycjonowanie (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy strateg product marketing w B2B dla usług profesjonalnych, analityk konkurencji (Polska), ekspert komunikacji compliance w zawodach regulowanych, badacz potrzeb użytkowników (doradcy podatkowi).
- Domena biznesowa: konkurencyjne pozycjonowanie Polish Tax Advisor GPT na rynku narzędzi dla doradców podatkowych w Polsce.
- Cel strategiczny: zdefiniować defensywną unikalną propozycję wartości wobec alternatyw, opartą o realne unmet needs i dowody; przygotować bezpieczne komunikaty marketingowe zgodne z ograniczeniami (brak obietnic porad/nieomylności).
- Aktualny stan: istnieją alternatywy (oprogramowanie, bazy prawne, manualny research, ogólne LLM), ale brakuje jednoznacznego „dlaczego teraz” i „dlaczego my” dla doradców podatkowych.
- Decyzje zależne: messaging, landing page, argumenty sprzedażowe, materiały proof points, segmenty docelowe.

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz.
- Preferuj dowody i źródła polskie; analogi UE/świat oznacz jako analog.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie: jaka jest defensywna, unikalna propozycja wartości Polish Tax Advisor GPT względem istniejących alternatyw, zgodna z wymogami compliance i oczekiwaniami doradców?
- Sub-pytania do pokrycia:
  - jakie alternatywy dominują (software podatkowo-księgowy, bazy prawne, arkusze, manualny research, konsultacje)?
  - jakie są unmet needs i frustracje (czas researchu, nadążanie za zmianami, dokumentowanie podstawy, jakość, stres compliance)?
  - jakie unikalne możliwości wnosi konwersacyjny interfejs AI (workflow, synteza, checklists, cytowania, audytowalność)?
  - jakie statementy rezonują, a jakie są ryzykowne regulacyjnie/reputacyjnie?
- Hipotezy do weryfikacji:
  - H1: oszczędność czasu researchu jest kluczową propozycją wartości.
  - H2: interfejs konwersacyjny jest istotnym wyróżnikiem.
  - H3: transparentność cytowań odróżnia od ogólnych LLM.
- Kontrargumenty:
  - różnica może wynikać bardziej z compliance i audytowalności niż z samej konwersacji.
  - doradcy mogą postrzegać konwersacyjność jako „zabawę”, a wartość widzieć w integracjach i workflow.

3. PARAMETRY RYNKU I OPERACJI
- Rynek: Polska, doradcy podatkowi; uwzględnij segmentację (solo, małe, średnie).
- Konkurencja i substytuty:
  - systemy informacji prawnej, narzędzia podatkowe/księgowe, szkolenia/biuletyny, ogólne LLM, własne playbooki.
- Dane wymagane:
  - badania potrzeb i zachowań doradców/księgowych/prawników w Polsce,
  - przegląd ofert alternatyw (funkcje, ceny, pozycjonowanie) wraz ze źródłami,
  - dowody na unmet needs (raporty, ankiety, artykuły instytucjonalne, publikacje branżowe z metodyką).
- Ograniczenia compliance dla komunikacji:
  - unikaj stwierdzeń sugerujących udzielanie porad podatkowych zamiast doradcy,
  - unikaj obietnic nieomylności, gwarancji poprawności, przejęcia odpowiedzialności.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: analiza konkurencji + propozycje UVP + rekomendowane pozycjonowanie + lista ryzykownych claimów.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1
  - [ ] Tier 2
  - [X] Tier 3
- Wymagane elementy:
  - [X] Competitive Positioning Analysis (SWOT lub równoważne)
  - [X] Customer Insights & Pain Points
  - [X] Risk Assessment & Mitigation Strategies (messaging/compliance)
  - [X] Implementation Roadmap with Milestones (jak zebrać proof points, test messaging)
  - [X] [Other: „don’t say list” i bezpieczne zamienniki]
- Język: polski.

Wymagany format wyjścia:
1) UVP Options: 3–5 wariantów (każdy jako krótki statement) + dowody/uzasadnienie i cytowania do unmet needs oraz do różnicowania.
2) Tabela różnicowania: Wyróżnik | Unmet need | Dlaczego AI to rozwiązuje | Zastrzeżenia compliance i ryzyka | Jak to udowodnić (artefakt).
3) Rekomendowane pozycjonowanie: jeden główny statement + 5–7 proof points + lista „nie używaj” (ryzykowne sformułowania) oraz bezpieczne alternatywy.
4) Ocena pewności: co jest dobrze podparte danymi, a co wymaga testu (message testing, wywiady); pytania do wywiadów.

Wymogi dowodowe:
- Unmet needs muszą mieć cytowane dowody. Jeśli dowodów PL brakuje, oznacz to i użyj analogów z jasnym zastrzeżeniem.
- Nie formułuj komunikatów, które mogą zostać odczytane jako świadczenie usług doradztwa podatkowego przez system.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Tier 1: polskie raporty i badania z metodyką, publikacje instytucji i organizacji branżowych, wiarygodne źródła rynkowe.
- Tier 2: analizy UE/świat dot. AI w usługach profesjonalnych, raporty z metodyką.
- Tier 3: strony konkurentów i materiały marketingowe (tylko jako dane o pozycjonowaniu konkurencji, z oznaczeniem stronniczości).

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Bias: aktywnie szukaj dowodów przeciwko hipotezom H1–H3.
- Konfrontacja z substytutami: pokaż, kiedy alternatywy są lepsze i dlaczego; wyprowadź z tego „defensywny” UVP.
- Scenariusze: jak UVP zmienia się w zależności od segmentu i dojrzałości cyfrowej.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Plan testowania messaging: testy A/B, wywiady, pilotaż marketingowy; metryki skuteczności komunikatów.
- KPI: rozumienie wartości, zaufanie, intencja zakupu, obawy compliance, aktywacja w pilocie.
- Iteracja: jak aktualizować UVP wraz z rozwojem funkcji i zmianami regulacji.


PROMPT 1 (Q7) — Bariery adopcji i strategie (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy analityk badań rynku technologii dla usług profesjonalnych (Polska), ekspert zarządzania zmianą w zawodach regulowanych, specjalista zaufania do AI i wdrożeń pilotażowych, konsultant ds. ryzyk regulacyjnych (RODO, tajemnica zawodowa doradcy podatkowego, odpowiedzialność).
- Domena biznesowa: adopcja narzędzi AI w polskich usługach doradztwa podatkowego (doradcy podatkowi, kancelarie doradztwa podatkowego), z naciskiem na bariery zaufania, ryzyka prawne, poufność, integrację w workflow i projekt pilota.
- Cel strategiczny: zaprojektować strategię adopcji i plan pilota (60–90 dni) dla Polish Tax Advisor GPT, minimalizując ryzyka (poufność, odpowiedzialność, zgodność) oraz maksymalizując szybkość i trwałość wdrożenia w segmencie sceptycznych profesjonalistów.
- Aktualny stan wiedzy: wcześniejsze prace (Tiers 1–4) zakończone; obecny etap to pogłębione doprecyzowanie barier, ich skali oraz skutecznych strategii przełamywania w realiach Polski.
- Luki rynkowe i szansa: brak klarownych, udokumentowanych i „auditowalnych” praktyk wdrażania asystentów AI w doradztwie podatkowym w Polsce; możliwość zbudowania zaufania poprzez dowody, mechanizmy kontroli oraz partnerstwa branżowe.
- Wpływ na decyzje: wyniki mają bezpośrednio zasilić strategię produktu i go-to-market: komunikacja, onboarding, dowody wartości, compliance-by-design, konstrukcja pilota i kryteria sukcesu.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Nie opieraj wniosków na źródłach opublikowanych po tej dacie; jeśli je znajdziesz, oznacz jako „poza zakresem (po cutoff)” i nie używaj do kluczowych tez.
- Preferuj dowody i źródła polskie. Gdy brakuje danych PL, użyj analogów UE/świat i jednoznacznie je oznacz jako analog (wraz z krótkim uzasadnieniem transferowalności lub jej braku).

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie strategiczne: jakie są główne bariery adopcji narzędzi AI przez polskich doradców podatkowych i jakie strategie (komunikacja, proof points, pilotaże, mechanizmy zgodności) je przełamują?
- Hipotezy robocze do weryfikacji (potwierdź lub obal dowodami):
  - H1: obawy o odpowiedzialność zawodową są barierą nr 1 (przed kosztem i użytecznością).
  - H2: poparcie/endorsement KIDP/KRDP lub równoważnej instytucji branżowej istotnie przyspiesza adopcję.
  - H3: demonstracja trafności wraz z cytowaniami i śladem źródłowym buduje zaufanie szybciej niż referencje i testimonials.
- Kontrargumenty konkurencyjne i alternatywne wyjaśnienia:
  - doradcy mogą obawiać się przede wszystkim poufności/tajemnicy zawodowej, a nie odpowiedzialności.
  - kluczową barierą może być brak czasu na naukę, integracja z praktyką kancelarii i narzędziami, a nie zaufanie.
  - endorsement instytucji może być mniej istotny niż „peer proof” od wiodących kancelarii lub wymogi klientów korporacyjnych.
- Ramy analityczne:
  - Jobs-to-be-Done dla doradców podatkowych (co próbują osiągnąć, gdzie AI pomaga, gdzie przeszkadza).
  - Modele adopcji technologii: TAM/UTAUT, Diffusion of Innovations.
  - Zarządzanie zmianą: ADKAR lub równoważne.
  - Analiza ryzyka: ryzyko merytoryczne, prawne, reputacyjne, operacyjne, bezpieczeństwa informacji.

3. PARAMETRY RYNKU I OPERACJI
- Rynek docelowy: licencjonowani doradcy podatkowi w Polsce oraz kancelarie doradztwa podatkowego.
- Segmentacja do zastosowania w analizie (zawsze rozdzielaj wnioski, jeśli źródła na to pozwalają):
  - praktyka jednoosobowa,
  - mała kancelaria (2–10),
  - średnia/większa kancelaria,
  - specjalizacje: VAT, CIT, PIT, ceny transferowe, MDR, spory, KSeF.
- Krajobraz alternatyw i substytutów: komercyjne systemy informacji prawnej, oprogramowanie podatkowo-księgowe, manualny research w źródłach MF/ISAP/orzecznictwo, ogólne LLM, szkolenia/biuletyny.
- Horyzont czasowy: strategia adopcji i pilota 60–90 dni, z perspektywą skalowania w 6–12 miesięcy.
- Otoczenie regulacyjne i ograniczenia: tajemnica zawodowa i standardy zawodowe doradców podatkowych, RODO i bezpieczeństwo informacji, odpowiedzialność cywilna/kontraktowa, zasady dokumentowania pracy.
- Wymagane dane i typy dowodów:
  - badania ankietowe i raporty dotyczące AI w zawodach regulowanych w Polsce (prawnicy, księgowi, doradcy podatkowi),
  - stanowiska i publikacje organizacji branżowych,
  - artykuły naukowe o zaufaniu do AI w środowiskach profesjonalnych,
  - case studies wdrożeń w kancelariach prawnych/księgowych (Polska lub UE),
  - dane o incydentach, wyciekach, karach i postępowaniach dyscyplinarnych związanych z narzędziami cyfrowymi (jeśli dostępne).
- Etyka i reputacja: unikaj wniosków sugerujących „automatyzację odpowiedzialności” lub zastąpienie profesjonalnego osądu; uwzględnij ryzyko reputacyjne w razie błędu AI.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: praktyczny playbook wdrożeniowy dla leadershipu produktu, z wyraźnym rozdziałem „dowody PL” vs „analogi”.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1: podsumowanie dla zarządu (1–2 strony)
  - [ ] Tier 2: analiza operacyjna (5–8 stron)
  - [X] Tier 3: kompleksowy business case z ryzykami, segmentacją i planem pilota
- Wymagane elementy:
  - [X] Customer Insights & Pain Points
  - [X] Risk Assessment & Mitigation Strategies
  - [X] Implementation Roadmap with Milestones
  - [X] Organizational Impact & Change Management
  - [ ] Market Size & Growth Projections (uwzględnij tylko jeśli znajdziesz solidne dane)
  - [ ] Financial Model & ROI Projections (tylko jeśli masz dane do sensownych założeń; w przeciwnym razie opisz, jak je zebrać)
  - [X] [Other: matryca barier i artefaktów dowodowych]
- Wymagania dot. wizualizacji: jedna tabela „Barrier-to-Mitigation” oraz tabelaryczny plan pilota.
- Odbiorcy: product leadership, compliance lead, head of growth/marketing, customer success.
- Język: całość po polsku.

Wymagany format wyjścia (po polsku, zachowaj kolejność i strukturę):
1) Barrier Summary: 8–12 punktów, każdy punkt z przypisem/cytowaniem do źródła (link, data, instytucja). Jeśli twierdzenie jest wnioskowaniem, oznacz „wniosek syntetyczny” i podeprzyj go co najmniej dwoma źródłami.
2) Tabela Barrier-to-Mitigation: Bariera | Dowód (PL/UE/Global + cytaty) | Nasilenie (1–5) | Mitigacja | Artefakt dowodowy (co pokazujemy użytkownikowi).
3) Plan pilota 60–90 dni: cele, metryki sukcesu, kontrola zgodności, onboarding, pętla feedbacku, kryteria „stop/go”, komunikacja ryzyk i eskalacje.
4) Ocena pewności: dla każdej głównej bariery wskaż pewność (wysoka/średnia/niska), kluczowe niewiadome oraz lista pytań do wywiadów (12–20 pytań) z doradcami i partnerami kancelarii.

Zasady cytowania i weryfikacji w raporcie:
- Każde twierdzenie o „częstości”, „skali”, „najczęściej”, „nr 1” wymaga cytowanego źródła. Jeśli brak danych, napisz wprost „brak danych ilościowych w Polsce” i nie zastępuj tego liczbami.
- Rozdzielaj: dowód ilościowy, dowód jakościowy, opinia ekspercka, materiał marketingowy.
- Jeśli źródła są sprzeczne, pokaż oba i wyjaśnij rozbieżność.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet źródeł:
  - Tier 1 (najwyższy): polskie źródła instytucjonalne i branżowe (organizacje samorządowe, regulatorzy, instytucje publiczne), recenzowane publikacje naukowe, raporty z jasną metodologią, wyniki badań ankietowych z ujawnioną próbą.
  - Tier 2: raporty UE, badania międzynarodowe dot. zawodów regulowanych, analizy firm badawczych i konsultingowych z metodologią.
  - Tier 3: prasa branżowa, blogi eksperckie, studia przypadków vendorów (tylko jako kontekst, z oznaczeniem ryzyka stronniczości).
- Wykluczenia: anonimowe fora, materiały bez metodologii, twierdzenia bez źródeł, dane przestarzałe (zwyczajowo starsze niż 3–5 lat, chyba że w Polsce brak nowszych).
- Wnioski z innych branż: prawnicy, audyt, księgowość, medycyna jako analogie wdrożeń w profesjach o wysokiej odpowiedzialności; oznacz analogiczność i ograniczenia.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Ocena siły dowodu: dla każdej bariery oznacz „mocne/umiarkowane/słabe” i wyjaśnij na podstawie typu źródła i metodologii.
- Ograniczanie biasów: aktywnie szukaj dowodów obalających hipotezy H1–H3; uwzględnij perspektywę sceptyków, nie tylko entuzjastów.
- Scenariusze adopcji: best/base/worst case z uzasadnieniem oraz warunkami brzegowymi (co musi się wydarzyć, aby scenariusz zaszedł).
- Analiza wrażliwości: pokaż, które elementy (endorsement, mechanizmy audytu, polityka poufności, integracje) najbardziej zmieniają decyzję adopcyjną.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Projekt pilota: zaproponuj konstrukcję pilota dla segmentu sceptycznych profesjonalistów, minimalizując ryzyko poufności i odpowiedzialności.
- KPI i metryki: adopcja, aktywność, czas do pierwszej wartości, wskaźniki zaufania, liczba eskalacji do człowieka, błędy wykryte przed wysyłką do klienta, wskaźniki zgodności.
- Pętle feedbacku: kanały, częstotliwość, format; jak zamieniać feedback na backlog produktu i zmiany polityk.
- Protokół iteracji: rytm przeglądów (co tydzień w pilocie, potem co miesiąc), kryteria rozszerzania funkcji i segmentów.


PROMPT 2 (Q14) — Zadania najbardziej podatne na błędy i wzorce prewencji (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy analityk zgodności podatkowej, specjalista analizy błędów (quality assurance) w doradztwie i księgowości, ekspert ds. kontroli podatkowych i wyników audytów, konsultant responsible AI dla zastosowań regulowanych.
- Domena biznesowa: identyfikacja i redukcja błędów w praktyce doradztwa podatkowego w Polsce, projektowanie bezpiecznych modułów „mistake prevention” dla Polish Tax Advisor GPT.
- Cel strategiczny: wskazać najbardziej error-prone zadania w polskiej praktyce podatkowej, opisać typowe kategorie pomyłek i konsekwencje, a następnie zaproponować bezpieczne wzorce wsparcia AI oraz obowiązkowe „human review triggers”.
- Aktualny stan wiedzy: istnieją rozproszone dane z kontroli, raportów organów, NIK, publikacji MF/KAS, orzecznictwa i komentarzy; potrzebna synteza z rozróżnieniem częstotliwości i dotkliwości.
- Luki i szansa: brak ustrukturyzowanej mapy „gdzie i dlaczego doradcy/firmy popełniają błędy” oraz jak narzędzia AI mogą zmniejszyć ryzyko bez wchodzenia w rolę udzielania porad zamiast profesjonalisty.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Źródła po tej dacie oznacz jako „po cutoff” i nie używaj do kluczowych tez.
- Preferuj dane polskie; jeśli brakuje ilościowych, użyj analogów UE/świat z wyraźnym oznaczeniem.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie strategiczne: które zadania doradcze i compliance w Polsce są najbardziej podatne na błędy oraz jak asystent AI może pomagać w ich prewencji w sposób bezpieczny i zgodny?
- Hipotezy do weryfikacji:
  - H1: VAT ma najwyższą częstość błędów spośród kategorii podatkowych.
  - H2: błędy proceduralne i terminowe występują częściej niż błędy merytorycznej interpretacji.
  - H3: wsparcie AI oparte o checklisty może bezpiecznie redukować błędy proceduralne.
- Kontrargumenty i alternatywne wyjaśnienia:
  - częstotliwość błędów może wynikać z intensywności kontroli w danym obszarze, a nie z faktycznej „podatności”.
  - „najbardziej kosztowne” błędy mogą nie być „najczęstsze”.
- Ramy analityczne:
  - Matryca ryzyka: częstotliwość vs dotkliwość konsekwencji.
  - Klasyfikacja błędów: obliczeniowe, interpretacyjne, proceduralne, terminowe, dowodowe/dokumentacyjne, komunikacyjne.
  - Wzorce prewencji AI: checklisty, walidacje, wykrywanie braków, alerty terminów, weryfikacja źródeł, porównania z regułami, generowanie pakietu roboczego.

3. PARAMETRY RYNKU I OPERACJI
- Zakres praktyki: doradztwo podatkowe w Polsce, w tym rozliczenia VAT, CIT, PIT, JPK, raportowania, wnioski, interpretacje, korespondencja z organami, przygotowanie dokumentów i analiz.
- Źródła danych dla „error rates” i wzorców błędów:
  - raporty organów podatkowych i kontroli, sprawozdania instytucji, raporty NIK, publikacje MF/KAS, komunikaty i statystyki, jeśli dostępne publicznie,
  - omówienia typowych błędów w opracowaniach branżowych o ujawnionej metodologii,
  - orzecznictwo i interpretacje wskazujące powtarzalne obszary sporów i korekt,
  - badania/raporty dotyczące jakości w księgowości i compliance (Polska lub analog UE).
- Ograniczenia: brak publicznych, jednoznacznych statystyk dla części obszarów; w takim przypadku należy przejść na „dowody pośrednie” i jasno oznaczyć.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: raport produktowo-zgodnościowy z rekomendacjami modułów prewencji.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1: podsumowanie (1–2 strony)
  - [ ] Tier 2: analiza operacyjna (5–8 stron)
  - [X] Tier 3: kompleksowa analiza z matrycą ryzyka i rekomendacjami modułów MVP
- Wymagane elementy:
  - [X] Customer Insights & Pain Points (błędy i ich przyczyny w praktyce)
  - [X] Risk Assessment & Mitigation Strategies (jak AI zmniejsza ryzyko i gdzie może je zwiększać)
  - [X] Implementation Roadmap with Milestones (moduły MVP i walidacja)
  - [X] [Other: taxonomy błędów + human review triggers]
- Język: po polsku.

Wymagany format wyjścia:
1) Lista zadań najbardziej podatnych na błędy: Top 10 z krótkim opisem, konsekwencjami i cytowaniami (każda pozycja musi mieć dowód; jeśli nie ma ilościowego, dopuszczalne jest jakościowe z jasnym oznaczeniem).
2) Tabela taksonomii błędów: Zadanie | Najczęstsze błędy | Konsekwencja (kary, odsetki, korekty, ryzyko sporu, reputacja) | Dowód (źródło) | Wzorzec prewencji GPT | Wyzwalacz obowiązkowej weryfikacji przez człowieka.
3) Propozycje modułów „Error-Prevention” (3–6) dla MVP: opis funkcji, wymagane dane, ograniczenia, ryzyka, jak mierzyć efekt (KPI).
4) Ocena pewności: co jest dobrze udokumentowane, gdzie brak danych, jakich danych/wywiadów potrzeba.

Zasady dowodowe:
- Twierdzenia o częstości, „najczęstsze”, „najwięcej” wymagają cytatu. Nie twórz liczb ani rankingów bez źródeł.
- Przy każdym wzorcu prewencji oznacz: „wymaga walidacji” jeśli dotyczy merytorycznej interpretacji lub obszarów o wysokiej niepewności.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet źródeł:
  - Tier 1: polskie źródła publiczne o charakterze instytucjonalnym i raportowym, orzecznictwo i interpretacje jako dowód „powtarzalności problemu”, dokumenty z jawną metodyką.
  - Tier 2: publikacje naukowe, raporty branżowe z metodyką, analogi UE.
  - Tier 3: artykuły branżowe i komentarze ekspertów (tylko jako kontekst).
- Wykluczenia: materiały bez autorstwa/metodyki, sensacyjne wpisy, dane niezweryfikowane.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Triangulacja: dla topowych zadań szukaj potwierdzeń w co najmniej dwóch niezależnych źródłach.
- Rozdziel: „częstość” od „dotkliwości” i pokaż oba wnioski.
- Wskaż ryzyka wprowadzone przez AI: fałszywa pewność, błędne cytowania, nieaktualność prawa, halucynacje; zaproponuj kontrolki bezpieczeństwa.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Pilot walidacyjny modułów: jak testować redukcję błędów (próbki spraw, checklista jakości, audyt wewnętrzny).
- KPI: liczba wykrytych braków przed wysyłką, spadek korekt, czas pracy, satysfakcja i zaufanie.
- Pętla feedbacku: mechanizm zgłaszania błędów i uczenia bazy wiedzy, bez wprowadzania wrażliwych danych klienta poza bezpieczne kanały.


PROMPT 3 (Q16) — Potrzeby dostępu do informacji i architektura KB + Actions (Tier 5: Refinement)

1. KONTEKST TECHNICZNY I RAMY
- Eksperci prowadzący badanie: starszy architekt informacji (systemy wiedzy prawnej), inżynier systemów RAG/knowledge retrieval, specjalista ds. prywatności i bezpieczeństwa (RODO, tajemnica zawodowa), praktyk workflow doradztwa podatkowego.
- Domena techniczna: architektura źródeł wiedzy i dostępu do informacji dla Polish Tax Advisor GPT (baza wiedzy, wyszukiwanie, cytowania, Actions/integracje), z naciskiem na zgodność i minimalizację danych.
- Cel badania: zmapować typowe potrzeby informacyjne polskiego doradcy podatkowego oraz zaproponować architekturę „knowledge base + Actions”, która maksymalizuje użyteczność i zgodność (privacy-by-design, auditability-by-design).
- Aktualny stan zrozumienia: istnieją liczne źródła prawa i praktyki (ustawy, rozporządzenia, objaśnienia, interpretacje, orzecznictwo), a także zasoby komercyjne; brak jednolitego modelu ich klasyfikacji i sposobu bezpiecznego dostępu przez GPT.
- Luki i ryzyko: ryzyko nieaktualności, ryzyko niewłaściwego cytowania, ryzyko naruszenia poufności przy danych klienta i zasobach firmowych.
- Potencjalny wpływ: decyzje o tym, co wbudować w KB, co pobierać na żądanie, jakie Actions są konieczne w MVP, oraz jakie kontrolki prywatności wdrożyć.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Nie opieraj tez na źródłach po tej dacie; oznacz je jako „po cutoff”.
- Preferuj źródła polskie; dla prawa UE i orzecznictwa unijnego używaj źródeł oficjalnych UE.
- Język raportu: polski.

2. KLUCZOWE PYTANIE TECHNICZNE I HIPOTEZY
- Główne pytanie techniczne: jakich typów informacji potrzebuje polski doradca podatkowy i jaka architektura „KB + Actions” zapewni dostęp w sposób zgodny i praktyczny?
- Hipotezy do weryfikacji:
  - H1: baza interpretacji/wyjaśnień organów podatkowych (MF/KIS lub równoważne źródła instytucjonalne) jest jednym z najczęściej używanych zasobów zewnętrznych.
  - H2: dane klienta wymagają integracji Actions (pobieranie/operowanie na danych na żądanie), a nie wbudowania w wiedzę modelu.
  - H3: szablony i playbooki firmowe są bardzo wartościowe, ale trudne do integracji w sposób zgodny (klasyfikacja, uprawnienia, retencja, ślad audytowy).
- Alternatywne rozwiązania: pełny model „wyszukiwarka + cytowania bez KB”, rozwiązania hybrydowe (KB tylko dla stabilnego prawa, reszta on-demand), integracja z komercyjnymi systemami informacji prawnej.
- Podstawa teoretyczna: zasady privacy-by-design i data minimization, architektura RAG z cytowaniami i kontrolą wersji, zasady bezpieczeństwa informacji.

3. PARAMETRY METODOLOGICZNE
- Podejście: inwentaryzacja i klasyfikacja źródeł oraz potrzeb informacyjnych, mapowanie workflow doradcy podatkowego, ocena ryzyk prywatności i bezpieczeństwa dla każdej kategorii danych, ocena częstotliwości aktualizacji i dostępności technicznej (API, RSS, pobieranie stron).
- Typy dowodów: dokumenty instytucjonalne, opisy systemów źródłowych, regulaminy dostępu, publikacje branżowe o workflow, raporty o praktykach informacyjnych w zawodach prawniczych/podatkowych.
- Zakres czasowy: źródła do 17 stycznia 2026.
- Zakres systemów: zewnętrzne źródła prawa i praktyki, dane klienta, zasoby kancelarii, integracje narzędzi biurowych.

4. SPECYFIKACJA WYJŚCIA
- Struktura raportu: specyfikacja architektury informacji dla produktu z tabelami, klasyfikacją danych i rekomendacjami MVP.
- Streszczenie wymagane: Tak.
- Głębokość analizy:
  - [ ] Level 1: executive memo
  - [ ] Level 2: pełny przegląd (z diagramami)
  - [X] Level 3: kompleksowy raport z ryzykami, kontrolami i listą Actions (MVP + roadmap)
- Wymagane elementy:
  - [X] Architecture Overview
  - [X] Risk Matrix (privacy/security)
  - [X] Remediation Roadmap (kontrole i etapy)
  - [X] [Other: mapa potrzeb informacyjnych i metody dostępu]
- Wymagania dot. wizualizacji: jedna tabela architektoniczna oraz schemat logiczny (tekstowy) KB vs Actions.

Wymagany format wyjścia (po polsku):
1) Mapa potrzeb informacyjnych: wypunktowanie wg kategorii (prawo „statyczne”, praktyka i wytyczne „dynamiczne”, orzecznictwo, dane klienta, zasoby firmowe).
2) Tabela architektury: Kategoria informacji | Typowe źródła | Częstotliwość zmian | Metoda dostępu (KB, wyszukiwanie on-demand, Action/API, upload dokumentu) | Ryzyko prywatności (niske/średnie/wysokie) | Kontrola i zabezpieczenia (uprawnienia, logowanie, retencja, anonimizacja).
3) Rekomendowana architektura: proponowana struktura KB (taksonomia, wersjonowanie, zasady aktualizacji, cytowania) oraz lista Actions dla MVP (z krótkim opisem celu, danych wejściowych, minimalizacji danych, autoryzacji).
4) Ocena pewności: co opiera się na twardych źródłach, a co jest „estymacją praktyka”; lista pytań do wywiadów z doradcami i administratorami systemów w kancelariach.

Zasady weryfikacji:
- Twierdzenia o „najczęściej używanych źródłach” cytuj lub oznacz jako „estymacja praktyka” z uzasadnieniem.
- Dla każdej kategorii danych wymień ryzyka i kontrolki; jeśli nie jesteś pewien, oznacz „wymaga walidacji prawnej/bezpieczeństwa”.

5. HIERARCHIA DOWODÓW I JAKOŚĆ ŹRÓDEŁ
- Priorytet:
  - Tier 1: oficjalne źródła prawa (publikatory), oficjalne serwisy instytucji, oficjalne bazy interpretacji/objaśnień, oficjalne rejestry i portale; dokumentacja techniczna systemów i mechanizmów dostępu.
  - Tier 2: uznane systemy informacji prawnej i opracowania z metodyką, publikacje naukowe o systemach wiedzy prawnej.
  - Tier 3: komentarze branżowe i blogi (kontekst).
- Wykluczenia: materiały bez wskazania źródeł, treści przestarzałe bez oznaczeń.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Oznaczaj źródła jako „wiążące” vs „pomocnicze” dla praktyki doradcy.
- Sprawdzaj aktualność i wersję dokumentu; opisuj mechanizm „jak uniknąć nieaktualności” w architekturze.
- Dokumentuj niepewności i założenia (data minimization, retencja, dostęp).

7. PROTOKÓŁ WALIDACJI
- Analiza rozwiązań alternatywnych: co najmniej dwa warianty architektury (konserwatywny vs ambitny), z plusami/minusami.
- Symulacja review: zasugeruj checklistę przeglądu przez IOD/bezpieczeństwo oraz przez praktyka doradztwa podatkowego.
- Strategia reewaluacji: jak często przeglądać listę źródeł i Actions wraz ze zmianami prawa i narzędzi.


PROMPT 4 (Q18) — Ocena dojrzałości cyfrowej i onboarding (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy strateg customer success dla usług profesjonalnych, specjalista ds. dojrzałości cyfrowej, badacz adopcji technologii w Polsce, projektant onboardingu dla narzędzi regulowanych.
- Domena biznesowa: poziom digitalizacji pracy doradców podatkowych w Polsce oraz projektowanie infrastruktury wdrożeniowej i szkoleniowej dla Polish Tax Advisor GPT.
- Cel strategiczny: określić dojrzałość cyfrową segmentów doradców podatkowych, kluczowe narzędzia i procesy, oraz zaprojektować onboarding/training tracks minimalizujące barierę startu i ryzyko błędów.
- Aktualny stan wiedzy: istnieją wskaźniki digitalizacji administracji podatkowej (e-usługi, e-filing, JPK, KSeF), ale trzeba je przełożyć na dojrzałość praktyk i kancelarii.
- Decyzje, które zależą od wyników: treści szkoleniowe, format onboarding (szablony promptów, tutoriale), wymagania techniczne, segmentacja marketingowa, priorytety funkcji.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Źródła po tej dacie oznacz jako „po cutoff”.
- Preferuj dowody polskie; analogi UE/świat tylko gdy brak PL.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie strategiczne: jaki jest poziom dojrzałości cyfrowej polskich doradców podatkowych i jaka infrastruktura onboardingowo-szkoleniowa jest potrzebna, aby skutecznie wdrożyć Polish Tax Advisor GPT?
- Hipotezy do weryfikacji:
  - H1: adopcja e-rozliczeń i e-usług jest wysoka, co sugeruje bazowy komfort cyfrowy.
  - H2: młodsi doradcy mają wyższy komfort korzystania z AI/automatyzacji.
  - H3: onboarding oparty o gotowe szablony i scenariusze (biblioteka promptów i przykładów zastosowań) jest najskuteczniejszy.
- Kontrargumenty:
  - wysoka adopcja e-usług nie oznacza umiejętności pracy z narzędziami AI.
  - dojrzałość może zależeć bardziej od wielkości kancelarii i typu klientów niż od wieku.
- Ramy analityczne: model dojrzałości cyfrowej (poziomy 1–5), segmentacja person, analiza „workflow readiness”.

3. PARAMETRY RYNKU I OPERACJI
- Rynek docelowy: doradcy podatkowi w Polsce (samodzielni i kancelarie).
- Obszary do zbadania:
  - aktualnie używane narzędzia: systemy księgowe, e-usługi administracji, systemy informacji prawnej, pakiety biurowe, narzędzia do obiegu dokumentów, CRM, podpis elektroniczny, e-doręczenia.
  - wskaźniki infrastruktury: wykorzystanie e-usług podatkowych, JPK, KSeF i innych obowiązków cyfrowych, jeśli dostępne publicznie.
  - kompetencje: automatyzacja, praca na szablonach, bezpieczeństwo informacji, praca na źródłach.
- Dane wymagane: statystyki adopcji (jeśli istnieją), raporty branżowe, badania kompetencji cyfrowych, publikacje instytucji, dane o wdrożeniach KSeF/JPK w firmach, jeśli dostępne.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: raport onboardingowy z segmentacją i rekomendacjami ścieżek wdrożenia.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1
  - [ ] Tier 2
  - [X] Tier 3
- Wymagane elementy:
  - [X] Customer Insights & Pain Points
  - [X] Risk Assessment & Mitigation Strategies (ryzyko błędów, bezpieczeństwo, prywatność)
  - [X] Implementation Roadmap with Milestones (onboarding)
  - [X] Organizational Impact & Change Management
  - [X] [Other: wskaźniki dojrzałości + artefakty wsparcia]
- Język: polski.

Wymagany format wyjścia:
1) Podsumowanie dojrzałości cyfrowej: 8–10 punktów, z cytowaniami przy danych/statystykach i rozróżnieniem segmentów.
2) Tabela wskaźników dojrzałości: Wskaźnik | Dowód (źródło, rok, metodyka) | Implikacja dla wdrożenia | Artefakt wsparcia (co dostarczamy: tutorial, checklist, integracja).
3) Plan onboardingu: 2–4 ścieżki wg persony (rola, dojrzałość, typ kancelarii), z modułami szkoleniowymi, czasem trwania, ćwiczeniami, zasadami bezpieczeństwa i miernikami postępu.
4) Ocena pewności: obszary wymagające walidacji, brakujące dane, propozycja badań (ankieta/wywiady) i pytania.

Zasady dowodowe:
- Statystyki adopcji muszą mieć cytat do źródła (instytucja, data, link). Jeśli nie znajdziesz statystyk, nie twórz ich.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet:
  - Tier 1: dane instytucji publicznych i raporty oficjalne, badania z metodyką, publikacje organizacji branżowych.
  - Tier 2: raporty rynkowe i akademickie, analogi UE.
  - Tier 3: artykuły branżowe (kontekst).
- Wykluczenia: ogólne opinie bez metodyki, dane nieopisane.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Triangulacja: porównuj co najmniej dwa niezależne źródła dla kluczowych wniosków o dojrzałości.
- Segmentacja: jeśli dane nie pozwalają na segmentację, oznacz ograniczenie i zaproponuj jak ją zebrać.
- Scenariusze: wskaż ryzyka przy onboardingu (błędy, poufność, fałszywe zaufanie do AI) i kontrolki.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Pilot onboardingowy: jak przetestować ścieżki na małej grupie doradców.
- KPI: ukończenie onboardingu, aktywacja, pierwsza wartość, zaufanie, liczba eskalacji do człowieka, incydenty bezpieczeństwa.
- Feedback loop: cykliczne retrospektywy i aktualizacje materiałów szkoleniowych w oparciu o pytania użytkowników.


PROMPT 5 (Q19) — Progi opłacalności i pozycjonowanie cenowe (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy analityk propozycji wartości (B2B), specjalista ROI w usługach profesjonalnych, analityk rynku SaaS w Polsce, ekspert ds. ekonomiki kancelarii podatkowych.
- Domena biznesowa: wycena i uzasadnienie ROI dla Polish Tax Advisor GPT w segmencie doradców podatkowych w Polsce; projektowanie pozycjonowania cenowego i argumentów wartości.
- Cel strategiczny: ustalić progi oszczędności czasu/kosztów i oczekiwania zwrotu, które uzasadniają zakup; przygotować model ROI dla segmentów i rekomendacje pricing/packaging.
- Aktualny stan wiedzy: istnieją ogólne wzorce ROI w SaaS, ale potrzebna adaptacja do ekonomiki doradztwa podatkowego w Polsce i realiów kancelarii.
- Decyzje zależne: ceny MVP, pakiety, komunikacja ROI, proof points, program pilota wartości.

Ustawienia czasu i zakresu:
- Traktuj „dzisiaj” jako 17 stycznia 2026 (cutoff). Źródła po tej dacie oznacz i nie używaj do kluczowych tez.
- Preferuj dane polskie (PLN). Jeśli używasz EUR lub innych walut, podaj kurs i źródło kursu oraz datę.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie: jakie progi oszczędności czasu/kosztów uzasadniają adopcję Polish Tax Advisor GPT u polskich doradców podatkowych i jak pozycjonować cenę?
- Hipotezy do weryfikacji:
  - H1: standardem oczekiwań jest payback 3–6 miesięcy.
  - H2: oszczędność 2+ godzin tygodniowo uzasadnia abonament 50–100 EUR miesięcznie.
  - H3: pozycjonowanie oparte o wartość (czas, ryzyko, jakość) działa lepiej niż pozycjonowanie „taniej”.
- Kontrargumenty:
  - próg może zależeć od liczby spraw o wysokiej wartości i od ryzyka odpowiedzialności, nie od czasu.
  - doradcy mogą preferować rozliczenie za użytkownika, za kancelarię lub za wolumen, nie stały abonament.
- Ramy analityczne: value-based pricing, unit economics kancelarii, payback i analiza wrażliwości, segmentacja person.

3. PARAMETRY RYNKU I OPERACJI
- Segmenty/persony do modelu ROI (minimum 3):
  - doradca solo,
  - mała kancelaria,
  - średnia/większa kancelaria.
- Dane wymagane:
  - stawki godzinowe lub ekwiwalent wartości godziny pracy (z badań, raportów, cenników publicznych, jeśli rzetelne),
  - liczba godzin poświęcana na research, przygotowanie pism, weryfikacje, monitoring zmian,
  - koszty alternatyw (systemy informacji prawnej, szkolenia, asystenci),
  - oczekiwania zakupowe B2B: payback, budżety narzędziowe, akceptowane ceny w Polsce.
- Ograniczenia: jeśli dane o stawkach są rozproszone i niepewne, pokaż zakresy i źródła oraz wykonaj analizę wrażliwości.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: model ROI + rekomendacje pricing/positioning + proof points.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1
  - [ ] Tier 2
  - [X] Tier 3
- Wymagane elementy:
  - [X] Financial Model & ROI Projections (z jawnymi założeniami)
  - [X] Risk Assessment & Mitigation Strategies (ryzyko błędów, reputacji, compliance jako część wartości)
  - [X] Implementation Roadmap with Milestones (jak zebrać proof points)
  - [X] [Other: rekomendacje pricing/packaging i „jak to sprzedać zgodnie”]
- Język: polski.

Wymagany format wyjścia:
1) Podsumowanie progów ROI: kluczowe wnioski (w punktach), z cytowaniami dla oczekiwań rynkowych, a dla obliczeń z jawnymi założeniami.
2) Tabela modelu ROI: Persona | Ekonomika bazowa (założenia: stawka, liczba godzin, koszty narzędzi) | Wymagana oszczędność czasu | Maksymalna cena miesięczna | Dowód (źródła dla stawek/benchmarków) | Wrażliwość (co jeśli założenia zmienią się o ±20%).
3) Rekomendacje pozycjonowania ceny: warianty pakietów, zakotwiczenia wartości, „proof points” (jak mierzyć oszczędność i ryzyko) oraz komunikaty zgodne (bez obietnic nieomylności).
4) Ocena pewności: które założenia mają twarde dane, a które wymagają badań; lista pytań do wywiadów i propozycja krótkiej ankiety.

Zasady obliczeń i dowodów:
- Każda kalkulacja ma pokazywać: dane wejściowe, jednostki, źródło, wzór i wynik.
- Nie twórz stawek ani danych kosztowych bez źródła; jeśli brak, pokaż zakres i opisz, jak go pozyskać.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Priorytet:
  - Tier 1: polskie raporty i badania z metodyką, dane instytucjonalne, wiarygodne benchmarki B2B, publiczne dane rynkowe.
  - Tier 2: analizy UE/świat dot. ROI w usługach profesjonalnych, raporty rynkowe z metodologią.
  - Tier 3: prasa i blogi (kontekst).
- Wykluczenia: „pricing pages” bez kontekstu, tezy bez metodologii.

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Analiza wrażliwości: przetestuj wpływ kluczowych założeń na wynik (czas oszczędzony, wykorzystanie, stawka).
- Bias: nie zakładaj automatycznie, że oszczędność czasu = równoważna wartości pieniężnej; pokaż ograniczenia.
- Scenariusze: base/best/worst, plus warunki.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Pilot pomiaru ROI: jak mierzyć realne oszczędności i ryzyka w pilocie (time tracking, ankiety, audyt jakości).
- KPI: oszczędność czasu, spadek korekt, skrócenie cyklu przygotowania dokumentu, wzrost satysfakcji, spadek stresu compliance.
- Iteracja pricing: jak aktualizować cennik i pakiety po zebraniu dowodów.


PROMPT 6 (Q21) — Kadencja aktualizacji wiedzy i workflow utrzymania (Tier 5: Refinement)

1. KONTEKST TECHNICZNY I RAMY
- Eksperci prowadzący badanie: senior content operations manager (prawo/regulacje), specjalista monitoringu zmian legislacyjnych, architekt wersjonowania treści i kontroli jakości, praktyk compliance w produktach AI.
- Domena techniczna: monitoring zmian prawa podatkowego i praktyki organów w Polsce, projektowanie procesu aktualizacji wiedzy i SLA dla Polish Tax Advisor GPT.
- Cel badania: określić, jak często zmieniają się kluczowe źródła i jakie zdarzenia wymagają pilnych aktualizacji; zaproponować realistyczną kadencję aktualizacji i workflow utrzymania.
- Aktualny stan zrozumienia: mnogość źródeł (ustawy/rozporządzenia, objaśnienia, interpretacje, orzecznictwo, komunikaty), różna częstotliwość i waga zmian; potrzebna systematyka.
- Wpływ: decyzje o budżecie utrzymania, narzędziach monitoringu, strukturze bazy wiedzy, testach regresji i mechanizmach „freshness”.

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz „po cutoff”.
- Priorytet: Polska; w zakresie prawa UE i orzecznictwa używaj źródeł oficjalnych.

2. KLUCZOWE PYTANIE TECHNICZNE I HIPOTEZY
- Główne pytanie: jak często zmieniają się krytyczne wytyczne i praktyka podatkowa w Polsce oraz jaka kadencja utrzymania zapewni akceptowalną aktualność i minimalne ryzyko błędów w tax GPT?
- Hipotezy do weryfikacji:
  - H1: interpretacje i bieżące komunikaty wymagają monitoringu co najmniej tygodniowego; zmiany ustawowe można przeglądać w cyklu miesięcznym.
  - H2: dla większości krytycznych źródeł istnieją kanały monitoringu (RSS, newsletter, BIP, API, feed publikatora) możliwe do automatyzacji.
  - H3: kwartalny przegląd całościowy jest wystarczający dla większości treści, przy osobnym trybie „urgent” dla zdarzeń krytycznych.
- Alternatywne ujęcia: tematy o wysokiej zmienności (VAT/KSeF) mogą wymagać częstszych przeglądów; orzecznictwo może wpływać na interpretacje bez zmian ustawowych.

3. PARAMETRY METODOLOGICZNE
- Podejście: ilościowe i jakościowe mapowanie częstotliwości publikacji i typów zmian dla każdej klasy źródeł; identyfikacja zdarzeń wyzwalających „urgent update”; przegląd narzędzi monitoringu i dobrych praktyk wersjonowania.
- Dane i dowody: statystyki publikacji (jeśli dostępne), archiwa komunikatów, dokumentacja kanałów monitoringu, opisy procesów w organizacjach prawniczych/compliance.
- Zakres: kluczowe obszary podatkowe i compliance; źródła instytucjonalne, orzecznictwo, praktyka.

4. SPECYFIKACJA WYJŚCIA
- Struktura raportu: rekomendacja kadencji + tabela triggerów + workflow utrzymania.
- Streszczenie wymagane: Tak.
- Głębokość analizy:
  - [ ] Level 1
  - [ ] Level 2
  - [X] Level 3
- Wymagane elementy:
  - [X] Architecture Overview (jak monitorujemy i aktualizujemy)
  - [X] Risk Matrix (ryzyko nieaktualności vs wpływ)
  - [X] Remediation Roadmap (wdrożenie procesu)
  - [X] [Other: SLA, wersjonowanie, testy regresji]
- Język: polski.

Wymagany format wyjścia:
1) Rekomendowana kadencja aktualizacji: wg tematu i typu źródła (tabela lub lista), z cytowaniami do danych o częstotliwości publikacji tam, gdzie to możliwe.
2) Tabela triggerów: Zdarzenie | Źródło | Waga/Severity (1–5) | Działanie | SLA (czas reakcji) | Kontrola jakości.
3) Workflow utrzymania: role, odpowiedzialności, wersjonowanie, mechanizmy diff, testy regresji, zasady publikacji, procedura „rollback”.
4) Ocena pewności: gdzie są twarde dane, gdzie trzeba walidacji; lista braków i pytań do zespołu merytorycznego.

Wymogi weryfikacyjne:
- Twierdzenia o częstotliwości publikacji muszą mieć cytat lub wyraźne oznaczenie „brak danych, estymacja na podstawie próby” wraz z opisem próby.
- Zawsze weryfikuj dostępność mechanizmów monitoringu (RSS/API/newsletter) w źródłach.

5. HIERARCHIA DOWODÓW I JAKOŚĆ ŹRÓDEŁ
- Tier 1: oficjalne publikatory i serwisy instytucji (z metadanymi dat), oficjalne archiwa i bazy, dokumentacja techniczna kanałów informacyjnych.
- Tier 2: opracowania z metodyką, raporty analityczne o zmianach prawa.
- Tier 3: komentarze branżowe jako kontekst trendów.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Rejestrowanie wersji: wymagaj propozycji „jak identyfikować wersję prawa i wiedzy użytej do odpowiedzi”.
- Unikanie luk: opisz, jak wykrywać brak monitoringu dla kluczowych źródeł i jak to kompensować.
- Oznaczanie stanu: „aktualne na dzień …” i „wymaga weryfikacji” dla obszarów o wysokiej zmienności.

7. PROTOKÓŁ WALIDACJI
- Alternatywne rozwiązania: porównaj co najmniej dwa modele utrzymania (częste małe aktualizacje vs rzadkie duże).
- Peer review: zaproponuj review merytoryczne i compliance.
- Reewaluacja: zaproponuj cykl przeglądu procesu utrzymania (kwartalnie) i kryteria zmiany kadencji.


PROMPT 7 (Q22) — Priorytety integracji Actions/API (MVP vs roadmap) (Tier 5: Refinement)

1. KONTEKST TECHNICZNY I RAMY
- Eksperci prowadzący badanie: starszy architekt integracji (API), architekt bezpieczeństwa i prywatności, praktyk automatyzacji w kancelariach, analityk wartości vs wysiłek (value/effort).
- Domena techniczna: projektowanie i priorytetyzacja integracji (GPT Actions) dla Polish Tax Advisor GPT w środowisku kancelarii podatkowych w Polsce.
- Cel badania: określić, które integracje generują największą wartość (czas, ryzyko, jakość) przy akceptowalnej wykonalności i ryzyku, oraz zaproponować MVP vs roadmap.
- Aktualny stan: wiele potencjalnych integracji (źródła prawa, e-usługi, systemy księgowe, DMS, poczta, kalendarz); potrzebna macierz priorytetów i minimalna architektura bezpieczeństwa.
- Wpływ: backlog produktu, plan prac technicznych, wymagania partnerstw i zgodności.

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz.
- Preferuj informacje z dokumentacji API i źródeł instytucjonalnych lub vendorów.

2. KLUCZOWE PYTANIE TECHNICZNE I HIPOTEZY
- Główne pytanie: które integracje Actions dają największą wartość doradcom podatkowym w Polsce, a które powinny trafić do MVP vs roadmap?
- Hipotezy do weryfikacji:
  - H1: integracja z oficjalnym źródłem aktów prawnych (ISAP lub równoważne) daje najwyższą wartość w research i cytowaniach.
  - H2: integracje ERP/systemów księgowych są wysokowartościowe, ale wysokozłożone (API, wdrożenia, różnorodność).
  - H3: integracja KSeF powinna być priorytetem roadmapy, a nie MVP (potwierdź stan regulacyjny i gotowość ekosystemu na cutoff).
- Alternatywy: MVP bez integracji z systemami transakcyjnymi (tylko read-only i źródła publiczne), integracje przez eksport/import plików, integracje przez DMS.

3. PARAMETRY METODOLOGICZNE
- Podejście: identyfikacja listy typowych narzędzi używanych w kancelariach, przegląd dostępności API i warunków licencyjnych, analiza wymagań autoryzacji, ocena ryzyk prywatności i tajemnicy zawodowej, scoring value vs effort.
- Dane: dokumentacja API, warunki korzystania, przykłady wdrożeń, opisy integracji, wymagania prawne dot. danych.

4. SPECYFIKACJA WYJŚCIA
- Struktura raportu: lista top integracji + tabela priorytetyzacji + rekomendowana architektura MVP.
- Streszczenie wymagane: Tak.
- Głębokość analizy:
  - [ ] Level 1
  - [ ] Level 2
  - [X] Level 3
- Wymagane elementy:
  - [X] Architecture Overview
  - [X] Security Assessment
  - [X] Risk Matrix
  - [X] Remediation Roadmap
  - [X] [Other: tabela value/effort + wymagania vendor partnership]
- Język: polski.

Wymagany format wyjścia:
1) Top Actions Shortlist: krótka lista (z uzasadnieniem wartości i ograniczeń), z cytowaniami dot. dostępności API lub mechanizmów dostępu.
2) Tabela priorytetyzacji: Action | Wartość (1–5) + uzasadnienie | Wykonalność (1–5) + dowód z dokumentacji | Warunki wstępne | Ryzyko zgodności/prywatności (niske/średnie/wysokie) | Priorytet (MVP/Roadmap/Do not do) | Uwagi o partnerstwie.
3) Rekomendowana architektura MVP: minimalizacja danych, autoryzacja (preferuj OAuth/bezpieczne tokeny), logowanie i audyt, zakresy uprawnień, zasady przechowywania, izolacja tenantów, mechanizm wycofania zgody, obsługa incydentów.
4) Ocena pewności: które integracje mają jasną dokumentację i stabilne API, a które wymagają negocjacji lub walidacji; lista pytań do vendorów.

Wymogi weryfikacyjne:
- Wykonalność ma być podparta cytatem z dokumentacji API/warunków użycia.
- Oznacz integracje wymagające partnerstwa vendorowego, płatnych licencji lub ograniczeń prawnych.

5. HIERARCHIA DOWODÓW I JAKOŚĆ ŹRÓDEŁ
- Tier 1: oficjalna dokumentacja API, regulaminy, specyfikacje, oficjalne repozytoria, komunikaty instytucji.
- Tier 2: analizy techniczne i wdrożeniowe z metodologią, case studies.
- Tier 3: blogi i dyskusje techniczne jako kontekst, z ostrzeżeniem o wiarygodności.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Spójne kryteria scoringu: zdefiniuj, co oznacza 1–5 dla wartości i wykonalności.
- Minimalizacja danych: dla każdej integracji wskaż minimalny zestaw danych i ryzyko.
- Unikaj wniosków bez dowodów: jeśli API nie jest potwierdzone, oznacz „niepotwierdzone”.

7. PROTOKÓŁ WALIDACJI
- Porównaj co najmniej dwa warianty MVP: (A) tylko źródła publiczne + cytowania, (B) źródła publiczne + 1–2 integracje produktowo-kluczowe.
- Peer review: review bezpieczeństwa i review praktyka kancelarii.
- Reewaluacja: kwartalna rewizja priorytetów integracji wraz ze zmianami ekosystemu.


PROMPT 8 (Q24) — Mechanizmy weryfikacji zgodności i audytowalności (Tier 5: Refinement)

1. KONTEKST PRAWNY I RAMY
- Eksperci prowadzący badanie: prawnik/compliance officer w obszarze produktów AI, ekspert ds. standardów audytu i śladu dowodowego, specjalista ds. ochrony danych (RODO), praktyk doradztwa podatkowego (dokumentacja pracy, odpowiedzialność).
- Domena prawna: wymagania audytowalności i weryfikacji pracy wspieranej przez AI w doradztwie podatkowym w Polsce, w tym standardy dokumentacji, transparentność źródeł, kontrola wersji bazy wiedzy i logowanie.
- Cel badania: zidentyfikować mechanizmy weryfikacji i dowodzenia, które doradca podatkowy oraz kancelaria powinni mieć, aby bronić jakości i zgodności pracy wspieranej przez AI (z uwzględnieniem wymogów prawa, standardów i dobrych praktyk).
- Jurysdykcja: Polska + prawo UE mające zastosowanie (RODO, regulacje dotyczące AI i usług cyfrowych, jeśli relewantne).
- Aktualny stan wiedzy: istnieją ogólne standardy compliance (bezpieczeństwo, privacy, audit trails), ale trzeba je przełożyć na praktykę doradztwa podatkowego i defensibility outputów.
- Luki: niejednoznaczność wymogów co do logów, retencji, formy „work papers” dla interakcji z AI; potrzeba mapowania na źródła prawne i standardy.
- Potencjalny wpływ: decyzje produktowe (co logować, jak wersjonować, jak eksportować „pakiet audytowy”, polityki retencji i zgód).

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz „po cutoff”.
- Preferuj źródła oficjalne i wiążące; materiały vendorów tylko jako kontekst.

2. GŁÓWNE PYTANIE PRAWNE I HIPOTEZY
- Główne pytanie: jakie mechanizmy weryfikacji zgodności i audytowalności są niezbędne, aby audytować i bronić pracy doradcy podatkowego wspieranej przez AI?
- Hipotezy do weryfikacji:
  - H1: cytowanie konkretnych źródeł prawa i praktyki (z identyfikatorem wersji/daty) jest wystarczające dla defensibility audytowej w większości przypadków.
  - H2: doradcy potrzebują eksportowalnych „work papers” z interakcji z AI (pakiet audytowy).
  - H3: wersja bazy wiedzy użyta w momencie zapytania musi być logowana.
- Alternatywne interpretacje:
  - same cytowania mogą nie wystarczać bez kontekstu rozumowania i dokumentacji decyzji człowieka.
  - retencja i logowanie mogą podlegać ograniczeniom RODO i zasadom minimalizacji danych.

3. PARAMETRY METODOLOGICZNE
- Projekt badania: przegląd prawa i standardów (analiza normatywna), mapowanie wymogów na mechanizmy techniczne, analiza porównawcza dobrych praktyk w audycie (w tym w profesjach regulowanych).
- Materiały: ustawy i akty wykonawcze, dokumenty regulacyjne UE, wytyczne organów i samorządów zawodowych, standardy bezpieczeństwa/audytu (jeśli stosowane w takich produktach), publikacje eksperckie z jasno wskazaną podstawą.
- Metody analityczne: interpretacja przepisów, rozróżnienie „wymóg prawny” vs „dobra praktyka”, analiza ryzyk i zgodności z zasadami RODO.

4. SPECYFIKACJA WYJŚCIA PRAWNEGO
- Struktura raportu: memorandum compliance + matryca mechanizmów audytowych.
- Abstract: Nie (zastąp to streszczeniem menedżerskim).
- Głębokość analizy:
  - [ ] Level 1: krótki opis
  - [ ] Level 2: standardowe memorandum
  - [X] Level 3: kompleksowa opinia/analiza z mapowaniem na mechanizmy produktowe
- Wymagane elementy:
  - [X] Issue Statement
  - [X] Rule(s) of Law
  - [X] Application/Analysis (mapowanie na mechanizmy techniczne)
  - [X] Conclusion
  - [X] Recommendations (product requirements i polityki)
  - [X] [Other: podział „wymóg prawny” vs „best practice” + ryzyka]
- Styl cytowań: cytowania „plain” z linkami i datą; dla aktów prawnych podaj publikator/źródło oficjalne.

Wymagany format wyjścia (po polsku):
1) Podsumowanie mechanizmów weryfikacji: lista mechanizmów (punkty) z krótkim uzasadnieniem i cytatami do podstawy (prawo/standard/best practice).
2) Tabela audytowalności: Mechanizm | Co logować | Retencja (propozycja + ograniczenia) | Artefakt (co powstaje) | Ryzyko (co jeśli brak) | Podstawa (wymóg prawny / best practice + cytat).
3) Szablony outputów: opis zawartości „pakietu audytowego” dla jednej sprawy (co ma zawierać, jak chronić dane, jak eksportować).
4) Ocena pewności: elementy wymagające walidacji prawnej (lista) i pytania do prawnika/inspektora ochrony danych/samorządu zawodowego.

Wymogi dowodowe:
- Każdy „wymóg” musi mieć cytat do źródła prawnego/standardu albo zostać oznaczony jako „best practice” z uzasadnieniem i źródłem.
- Mechanizmy potencjalnie kolidujące z RODO lub tajemnicą zawodową oznacz jako „ryzyko – wymaga decyzji prawnej”.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Tier 1: akty prawne i oficjalne wytyczne, regulacje UE, dokumenty organów publicznych, dokumenty samorządu zawodowego.
- Tier 2: uznane opracowania i standardy branżowe (bezpieczeństwo, audyt, zarządzanie AI) z jasno określonym zakresem.
- Tier 3: materiały kontekstowe (artykuły branżowe) tylko pomocniczo.

6. ZAPEWNIENIE JAKOŚCI I POWTARZALNOŚĆ
- Rozróżnij: co jest wiążące w Polsce, co jest perswazyjne lub rekomendowane.
- Wskazuj niepewności i obszary „unsettled”.
- Cytuj konkretny fragment/artykuł/sekcję, nie tylko stronę główną.

7. PROTOKÓŁ WALIDACJI
- Analiza alternatyw: co najmniej dwa warianty logowania/retencji (konserwatywny vs rozszerzony) z oceną ryzyk.
- Peer review: rekomenduj przegląd przez prawnika (podatki/RODO) oraz specjalistę bezpieczeństwa informacji.
- Strategia aktualizacji: zaproponuj, jak często weryfikować wymagania przy zmianach prawa i standardów.


PROMPT 9 (Q25) — Unikalna propozycja wartości (UVP) i pozycjonowanie (Tier 5: Refinement)

1. KONTEKST BIZNESOWY I RAMY DECYZYJNE
- Eksperci prowadzący badanie: starszy strateg product marketing w B2B dla usług profesjonalnych, analityk konkurencji (Polska), ekspert komunikacji compliance w zawodach regulowanych, badacz potrzeb użytkowników (doradcy podatkowi).
- Domena biznesowa: konkurencyjne pozycjonowanie Polish Tax Advisor GPT na rynku narzędzi dla doradców podatkowych w Polsce.
- Cel strategiczny: zdefiniować defensywną unikalną propozycję wartości wobec alternatyw, opartą o realne unmet needs i dowody; przygotować bezpieczne komunikaty marketingowe zgodne z ograniczeniami (brak obietnic porad/nieomylności).
- Aktualny stan: istnieją alternatywy (oprogramowanie, bazy prawne, manualny research, ogólne LLM), ale brakuje jednoznacznego „dlaczego teraz” i „dlaczego my” dla doradców podatkowych.
- Decyzje zależne: messaging, landing page, argumenty sprzedażowe, materiały proof points, segmenty docelowe.

Ustawienia czasu i zakresu:
- Cutoff: 17 stycznia 2026. Źródła po tej dacie oznacz.
- Preferuj dowody i źródła polskie; analogi UE/świat oznacz jako analog.

2. GŁÓWNE PYTANIE STRATEGICZNE I HIPOTEZY
- Główne pytanie: jaka jest defensywna, unikalna propozycja wartości Polish Tax Advisor GPT względem istniejących alternatyw, zgodna z wymogami compliance i oczekiwaniami doradców?
- Sub-pytania do pokrycia:
  - jakie alternatywy dominują (software podatkowo-księgowy, bazy prawne, arkusze, manualny research, konsultacje)?
  - jakie są unmet needs i frustracje (czas researchu, nadążanie za zmianami, dokumentowanie podstawy, jakość, stres compliance)?
  - jakie unikalne możliwości wnosi konwersacyjny interfejs AI (workflow, synteza, checklists, cytowania, audytowalność)?
  - jakie statementy rezonują, a jakie są ryzykowne regulacyjnie/reputacyjnie?
- Hipotezy do weryfikacji:
  - H1: oszczędność czasu researchu jest kluczową propozycją wartości.
  - H2: interfejs konwersacyjny jest istotnym wyróżnikiem.
  - H3: transparentność cytowań odróżnia od ogólnych LLM.
- Kontrargumenty:
  - różnica może wynikać bardziej z compliance i audytowalności niż z samej konwersacji.
  - doradcy mogą postrzegać konwersacyjność jako „zabawę”, a wartość widzieć w integracjach i workflow.

3. PARAMETRY RYNKU I OPERACJI
- Rynek: Polska, doradcy podatkowi; uwzględnij segmentację (solo, małe, średnie).
- Konkurencja i substytuty:
  - systemy informacji prawnej, narzędzia podatkowe/księgowe, szkolenia/biuletyny, ogólne LLM, własne playbooki.
- Dane wymagane:
  - badania potrzeb i zachowań doradców/księgowych/prawników w Polsce,
  - przegląd ofert alternatyw (funkcje, ceny, pozycjonowanie) wraz ze źródłami,
  - dowody na unmet needs (raporty, ankiety, artykuły instytucjonalne, publikacje branżowe z metodyką).
- Ograniczenia compliance dla komunikacji:
  - unikaj stwierdzeń sugerujących udzielanie porad podatkowych zamiast doradcy,
  - unikaj obietnic nieomylności, gwarancji poprawności, przejęcia odpowiedzialności.

4. SPECYFIKACJA WYNIKU RAPORTU
- Architektura raportu: analiza konkurencji + propozycje UVP + rekomendowane pozycjonowanie + lista ryzykownych claimów.
- Streszczenie menedżerskie: Tak.
- Głębokość analizy:
  - [ ] Tier 1
  - [ ] Tier 2
  - [X] Tier 3
- Wymagane elementy:
  - [X] Competitive Positioning Analysis (SWOT lub równoważne)
  - [X] Customer Insights & Pain Points
  - [X] Risk Assessment & Mitigation Strategies (messaging/compliance)
  - [X] Implementation Roadmap with Milestones (jak zebrać proof points, test messaging)
  - [X] [Other: „don’t say list” i bezpieczne zamienniki]
- Język: polski.

Wymagany format wyjścia:
1) UVP Options: 3–5 wariantów (każdy jako krótki statement) + dowody/uzasadnienie i cytowania do unmet needs oraz do różnicowania.
2) Tabela różnicowania: Wyróżnik | Unmet need | Dlaczego AI to rozwiązuje | Zastrzeżenia compliance i ryzyka | Jak to udowodnić (artefakt).
3) Rekomendowane pozycjonowanie: jeden główny statement + 5–7 proof points + lista „nie używaj” (ryzykowne sformułowania) oraz bezpieczne alternatywy.
4) Ocena pewności: co jest dobrze podparte danymi, a co wymaga testu (message testing, wywiady); pytania do wywiadów.

Wymogi dowodowe:
- Unmet needs muszą mieć cytowane dowody. Jeśli dowodów PL brakuje, oznacz to i użyj analogów z jasnym zastrzeżeniem.
- Nie formułuj komunikatów, które mogą zostać odczytane jako świadczenie usług doradztwa podatkowego przez system.

5. HIERARCHIA ŹRÓDEŁ I JAKOŚĆ
- Tier 1: polskie raporty i badania z metodyką, publikacje instytucji i organizacji branżowych, wiarygodne źródła rynkowe.
- Tier 2: analizy UE/świat dot. AI w usługach profesjonalnych, raporty z metodyką.
- Tier 3: strony konkurentów i materiały marketingowe (tylko jako dane o pozycjonowaniu konkurencji, z oznaczeniem stronniczości).

6. ZAPEWNIENIE JAKOŚCI I WALIDACJA STRATEGICZNA
- Bias: aktywnie szukaj dowodów przeciwko hipotezom H1–H3.
- Konfrontacja z substytutami: pokaż, kiedy alternatywy są lepsze i dlaczego; wyprowadź z tego „defensywny” UVP.
- Scenariusze: jak UVP zmienia się w zależności od segmentu i dojrzałości cyfrowej.

7. WDROŻENIE I CIĄGŁE DOSKONALENIE
- Plan testowania messaging: testy A/B, wywiady, pilotaż marketingowy; metryki skuteczności komunikatów.
- KPI: rozumienie wartości, zaufanie, intencja zakupu, obawy compliance, aktywacja w pilocie.
- Iteracja: jak aktualizować UVP wraz z rozwojem funkcji i zmianami regulacji.


PROMPT 1 (REFINED, Tier 6 / GPT Engineering): Conversation Design & UX Patterns

1. AI CONTEXT & RESEARCH FRAMEWORK
- TODAY / CUTOFF: Treat today as 17 January 2026. Use and cite only sources published on or before 17 January 2026. If a source page is updated after that date, treat it as out of scope unless you can verify the relevant content existed on or before 17 January 2026.
- Output Language: Write the entire report in Polish (including tables, labels, and templates).
- Product Context: Polish Tax Advisor GPT — AI decision-support tool for licensed Polish tax advisors (doradcy podatkowi). The GPT is intended to support professional analysis and verification workflows, not replace professional judgment.
- Expert(s) conducting the research: Senior Conversational AI Designer for regulated professional tools; Human–Computer Interaction (HCI) researcher specializing in trust in automation; LLM Product Engineer experienced with OpenAI Custom GPT constraints; UX Writer (Polish); Polish tax advisor subject-matter expert; Responsible AI / risk specialist.
- AI Domain: Conversational UX design patterns and interaction architecture for LLM-based decision-support in regulated tax advisory; Custom GPT interaction constraints and affordances.
- Research Objective: Produce an implementation-ready conversation design specification and pattern library for Polish Tax Advisor GPT, including reusable response templates, clarification and disambiguation frameworks, uncertainty communication patterns, and trust calibration mechanisms, all grounded in cited evidence and tailored to Custom GPT capabilities and constraints.
- Current AI Understanding: Tiers 1–5 are complete (assume earlier tiers defined user personas, top tax advisory tasks, content scope boundaries, and risk posture). If prior Tier 1–5 outputs are not provided, proceed with explicit assumptions and provide options.
- Knowledge Gaps & Technical Uncertainties:
  - How to maintain context robustly across multi-turn professional workflows while minimizing sensitive data capture
  - When to ask clarifying questions versus when to proceed with assumptions
  - How to communicate uncertainty and limitations in a way that calibrates trust and avoids automation bias
  - Which output formats best support verification and audit by tax professionals
  - How to implement these patterns within Custom GPT builder constraints (instructions, knowledge files, optional actions)
- Potential Impact: Higher answer quality, faster verification by advisors, reduced risk of overreliance, improved user satisfaction and adoption, lower incident rate from misinterpretation or missing context.

2. HYPOTHESIS & THEORETICAL FOUNDATION
- Primary AI Research Question: What conversation design patterns create the best professional user experience for licensed tax advisors using an AI tax advisory assistant while maintaining calibrated trust and minimizing risk?
- Working Hypotheses to Verify:
  - H1: Proactive clarification questions improve answer quality and reduce rework
  - H2: Structured output formats (tables, checklists, decision trees) are preferred by professionals and accelerate verification
  - H3: Explicit confidence/uncertainty indicators improve appropriate reliance and reduce automation bias
  - H4: Consistent response structure reduces cognitive load and error
- Alternative Theories / Counterpoints to Analyze:
  - Too many clarifying questions increase friction and reduce tool adoption
  - Confidence indicators can be misinterpreted and may increase overreliance if poorly designed
  - Strict templates may reduce flexibility for complex edge cases and exploratory analysis
- Theoretical Basis: Evidence-based conversational UX, cognitive load theory, trust calibration and automation bias literature, decision-support system usability research, regulated-domain conversational agent guidance, and documented best practices for professional verification workflows.

3. METHODOLOGICAL PARAMETERS
- Engineering / Research Approach:
  - Synthesize evidence from HCI, conversational UX, and trust-in-automation research
  - Benchmark interaction patterns from comparable professional assistants (legal, medical, accounting) focusing on verification workflows
  - Translate findings into an actionable pattern library and implementation playbook for a Custom GPT
  - Explicitly map each recommended pattern to (a) evidence, (b) intended effect, (c) implementation steps, (d) risks and mitigations, (e) suggested evaluation or A/B test
- Data & Evidence Required:
  - Peer-reviewed HCI and trust literature relevant to professional decision-support tools
  - Practitioner guidelines for conversational UX in professional settings
  - Official OpenAI documentation for Custom GPT capabilities/constraints as of cutoff
  - Evidence or guidance on communicating uncertainty and limitations in AI systems
- Timeframe: Sources up to 17 January 2026.
- Scope:
  - Supported usage: advisor-facing analysis, verification, drafting, and structured summarization for Polish tax topics (PIT, CIT, VAT, WHT, MDR, JPK, procedural questions) as defined by earlier tiers or assumed scope
  - Exclude: end-client direct advice mode, personal financial planning, non-Polish jurisdictions except EU law relevant to Polish taxes
- Sample Characteristics / Personas:
  - Primary user: licensed Polish tax advisor (time-constrained, risk-aware, needs legal basis and citations)
  - Secondary: tax manager / senior accountant (verification-focused)
- Ethics & Governance:
  - Require privacy-by-design interaction: data minimization, avoid collecting personally identifying data unless explicitly necessary
  - Include patterns that respect professional confidentiality and encourage advisors to avoid pasting sensitive client identifiers
  - Include explicit boundaries that the GPT is decision-support, not a substitute for professional judgment

4. OUTPUT SPECIFICATIONS
- Report Architecture: Technical UX specification + conversation pattern library + implementation playbook for a production Custom GPT.
- Abstract Required: Yes.
- Analytical Depth:
  - [ ] Level 1: Short technical note (2–4 pages)
  - [ ] Level 2: Full report (8–12 pages)
  - [X] Level 3: Comprehensive engineering report with copy-ready templates and implementation artifacts
- Required Elements (must be delivered in Polish and structured for implementation):
  - [X] Conversation Flow Patterns (table): Pattern | When to use | Steps | Example exchange | Implementation in Custom GPT | Risks | Citations
  - [X] Context Management Patterns: what to store, what not to ask, how to summarize and confirm context, how to handle long threads
  - [X] Clarification and Disambiguation Framework:
      - decision rules: when to ask vs proceed
      - minimal-question strategy
      - templates for clarifying questions in Polish
      - escalation rules for complex/unsafe cases
  - [X] Uncertainty Communication Guide (table):
      - Confidence level | User-facing phrasing in Polish | When to use | What to do next | Example | Citations
  - [X] Output Format Templates (copy-ready, in Polish):
      - “Szybka odpowiedź” template
      - “Analiza krok po kroku” template
      - “Checklist dla doradcy” template
      - “Tabela porównawcza” template
      - “Decyzja/ryzyko” template with red flags and verification steps
  - [X] Trust Calibration Patterns:
      - how to present citations and legal hierarchy
      - how to avoid overconfidence
      - limitation disclosures and “what I cannot determine” patterns
      - how to encourage verification without being obstructive
  - [X] Sample Conversations (3–5), annotated:
      - include at least one ambiguous user question, one complex multi-turn case, one “I cannot answer confidently” case, and one case that requires clarifying questions
      - annotate which pattern is used and why
  - [X] Implementation Artifacts (copy-ready):
      - a modular Custom GPT “Instructions” draft in Polish (sections that can be pasted into the builder)
      - a response-structure policy (how the GPT formats answers by default)
      - a lightweight intent-and-slots table for common tax advisor queries
      - a checklist for conversation QA and UX review
  - [X] Experimentation and A/B Testing Plan:
      - which patterns require testing
      - success metrics, test design, risks
  - [X] Confidence Assessment:
      - what is evidence-backed vs expert judgment
      - where evidence is limited or mixed
- Visualization Requirements: text-based flow diagrams or decision trees, tables, checklists; avoid images unless necessary.
- Target Audience: Product leadership, conversational design team, engineering/MLOps, compliance/risk, and tax SMEs.
- Citation Requirements: Provide citations for all major design recommendations and any factual claims; include a final bibliography and use consistent in-text numeric citations.

5. EVIDENCE HIERARCHY & SOURCE QUALITY
- Source Priority:
  - Tier 1: Peer-reviewed HCI, trust-in-automation research, decision-support usability studies; official OpenAI documentation; relevant standards/guidelines for uncertainty communication
  - Tier 2: High-quality industry research and practitioner handbooks with transparent methodology
  - Tier 3: Credible expert blogs only when Tier 1–2 are unavailable, and clearly labeled as lower confidence
- Source Exclusions: Unverified opinions, marketing-only content, sources without clear authorship/date, post-cutoff sources.

6. QUALITY ASSURANCE & REPRODUCIBILITY
- Evidence Grading: For each key recommendation, rate evidence strength (High/Medium/Low) and indicate source tiers used.
- Reproducibility Requirements: Provide copy-ready templates and decision rules that can be implemented without reinterpretation; include clear acceptance criteria for each pattern.
- Bias Assessment: Identify potential automation bias and confirmation bias risks created by each design.
- Uncertainty Quantification: Explicitly state where evidence is mixed and propose safe defaults.

7. RESEARCH VALIDATION PROTOCOL
- Negative Result Analysis: Identify patterns that can backfire and under what conditions.
- Alternative Solution Comparison: Compare at least two approaches for clarification strategy and for uncertainty display.
- Peer Review Simulation: Include a “skeptical reviewer” critique from a tax advisor and a safety/risk reviewer.
- Replication Strategy: Provide a plan to re-evaluate patterns after collecting real usage data and after material product changes.


====================


PROMPT 2 (REFINED, Tier 6 / GPT Engineering): Citation & Source Attribution

1. LEGAL CONTEXT & FRAMEWORK
- TODAY / CUTOFF: Treat today as 17 January 2026. Use and cite only sources published on or before 17 January 2026. If a source page is updated after that date, treat it as out of scope unless you can verify the relevant content existed on or before 17 January 2026.
- Output Language: Write the entire report in Polish.
- Product Context: Polish Tax Advisor GPT — decision-support for licensed Polish tax advisors. The system must support auditability, verification, and professional-grade sourcing for tax-law reasoning.
- Expert(s) conducting the research: Polish legal information specialist (prawo podatkowe); expert in Polish legal citation practice; knowledge architect for retrieval systems; legal operations/audit workflow designer; Responsible AI and compliance specialist.
- Legal Domain: Polish tax law and related procedural law; EU law relevant to Polish tax; citation standards for statutes, regulations, case law, administrative interpretations, and official guidance.
- Research Objective: Design a verifiable, professional-grade citation and source attribution system for AI-generated tax advisory content, including (a) a citation standard for outputs, (b) a knowledge-base encoding and metadata schema to preserve provenance at chunk/fragment level, (c) validation and audit workflows, and (d) conflict-resolution rules.
- Jurisdictional Scope: Poland (primary), EU (secondary where applicable).
- Current Legal Understanding: Official publication and reference systems exist for Polish law and practice materials (statutory journals, official databases, court judgments, administrative interpretations). A Custom GPT must translate these into consistent and verifiable citations.
- Knowledge Gaps & Opportunity:
  - Practical standards for AI-output citations that are both professional and efficient to verify
  - How to encode and retrieve citations reliably in a knowledge base used by a Custom GPT
  - How to validate citations at scale and prevent fabricated references
  - How to handle conflicting or uncertain sources while supporting professional verification workflows
- Potential Impact: Reduced risk of incorrect legal basis, faster advisor verification, defensible audit trail, improved trust calibration.

2. CORE LEGAL QUESTION & HYPOTHESIS
- Primary Legal Question: What citation approach ensures verifiable, professional-grade references for AI-generated Polish tax advisory content, suitable for licensed advisor verification and audit?
- Working Hypotheses to Verify:
  - H1: Inline citation anchors embedded in the knowledge base improve attribution accuracy
  - H2: A structured, standardized citation format improves verifiability and reduces ambiguity
  - H3: Direct links to authoritative sources enable efficient verification
  - H4: Indicating multi-source convergence increases appropriate confidence and supports auditability
- Alternative Interpretations / Counterarguments:
  - Overly dense citations reduce readability and may harm workflow adoption
  - Links alone are insufficient due to link rot or version drift; stable identifiers and journal references are required
  - Multi-source convergence can give false confidence if sources are non-independent or low authority

3. METHODOLOGICAL PARAMETERS
- Research Design:
  - Comparative analysis of Polish legal citation conventions for primary law, case law, and administrative guidance
  - Review of official publication/identification practices and recommended citation formats
  - Design of a metadata schema and citation encoding strategy for knowledge-base ingestion and retrieval (including chunk-level provenance)
  - Workflow design for validation, auditing, and ongoing maintenance (versioning, updates, link checking)
  - Mapping to Custom GPT constraints and implementation options (builder-only knowledge vs knowledge plus actions/backends)
- Data Types:
  - Statutory and regulatory texts and official publication references
  - Court decisions and official case repositories
  - Administrative interpretations and official tax guidance publications
  - Official documentation for the Custom GPT platform constraints relevant to citations and knowledge
- Temporal Scope: Up to 17 January 2026.
- Ethical & Privacy Protocols: Ensure citation workflow avoids storing or exposing sensitive client data; design for public-source citation plus optional internal-document provenance controls if used.

4. LEGAL OUTPUT SPECIFICATIONS
- Report Structure: Standard cytowania i atrybucji źródeł + specyfikacja techniczna (baza wiedzy, metadane, walidacja, audyt) dla Polish Tax Advisor GPT.
- Abstract Required: Yes.
- Analytical Depth:
  - [ ] Level 1: Brief summary (1–2 pages)
  - [ ] Level 2: Standard memo (3–5 pages)
  - [X] Level 3: Comprehensive standard + implementation specification with templates and decision trees
- Required Legal / Technical Elements:
  - [X] Summary of Polish Legal Citation Standards (table):
      - Typ źródła | Minimalny wymagany zapis | Zalecany pełny zapis | Przykład | Autoritatywne repozytorium | Uwagi o wersjonowaniu
      - Include at minimum: statutes/ustawy, regulations/rozporządzenia, consolidated texts/tekst jednolity, EU acts, court judgments (WSA/NSA, relevant EU court), administrative interpretations (KIS), general interpretations and official explanations (MF), official communications
  - [X] Authority Hierarchy and Weighting (table):
      - Rodzaj źródła | Moc wiążąca | Typowe zastosowanie | Ryzyko błędnej interpretacji | Jak prezentować w odpowiedzi
  - [X] Knowledge Base Citation Encoding:
      - Proposed metadata schema (field list with types and allowed values)
      - Chunk-level provenance model (chunk_id, source_id, location pointers, paragraph/article boundaries)
      - Canonical identifiers and stable references (journal positions, signatures, document IDs)
      - Versioning and amendment tracking strategy
  - [X] Citation Output Templates (copy-ready, in Polish):
      - Standard single-source citation template
      - Multi-source convergence template
      - “Source uncertain/needs verification” template
      - “Conflicting sources” template with explicit hierarchy reasoning
      - Template for quoting exact legal text with location pointer
  - [X] Validation Methodology (table):
      - Typ kontroli | Cel | Metoda | Automatyzacja | Częstotliwość | Kryteria zaliczenia | Co robić przy błędzie
      - Include link checking, identifier validation, citation-to-snippet alignment, and version currency checks
  - [X] Conflicting/Uncertain Source Handling:
      - Decision tree that prioritizes binding law and controlling authority
      - Rules for signaling conflicts to the advisor and requesting human verification
      - Escalation rules for legal review and “do not answer” conditions
  - [X] Implementation Mapping to Custom GPT Constraints:
      - What can be done with builder-only knowledge and instructions
      - What requires actions/backends (telemetry, link validation, internal document retrieval)
      - Risks and mitigations (prompt injection into citations, fabricated sources)
  - [X] Confidence Assessment:
      - Evidence-backed vs practice-based recommendations
      - Areas requiring legal review by a qualified Polish tax law expert
- Visualization Requirements: tables, schemas, and decision trees in text form.
- Target Audience: Product leadership, knowledge engineering, compliance, and tax SMEs.
- Citation Requirements: Cite official Polish legal guidance and authoritative repositories for citation practices; cite platform documentation for Custom GPT constraints; include bibliography.

5. EVIDENCE HIERARCHY & SOURCE QUALITY
- Source Priority:
  - Tier 1: Primary authority (official legal texts, official journals, official repositories, official court databases, official administrative interpretation databases)
  - Tier 2: Authoritative secondary (recognized legal publishers, bar/tax advisor association materials, academic legal writing guides)
  - Tier 3: Practitioner materials only when necessary and clearly labeled
- Source Exclusions: Unofficial compilations without provenance, outdated or superseded texts, post-cutoff updates.

6. QUALITY ASSURANCE & REPRODUCIBILITY
- Evidence Grading: Mark each standard/template with evidence tier and whether it is common practice vs explicitly stated in official guidance.
- Reproducibility Requirements: Provide templates and schemas that can be implemented directly; include sample JSON/YAML schemas where helpful.
- Bias Assessment: Identify risks of selective sourcing, non-authoritative sources, and “citation laundering.”
- Uncertainty Quantification: Explicitly label uncertain areas and propose conservative defaults.

7. RESEARCH VALIDATION PROTOCOL
- Negative Result Consideration: Describe failure modes such as link rot, version drift, and incomplete provenance.
- Alternative Interpretation Analysis: Compare at least two citation presentation styles (dense inline vs footnote summary) with pros/cons.
- Peer Review Simulation: Include review notes from a tax law SME and a knowledge engineer.
- Replication Strategy: Provide a maintenance plan for updates, periodic audits, and regression testing of citation templates.


====================


PROMPT 3 (REFINED, Tier 6 / GPT Engineering): Hallucination Prevention & Accuracy

1. AI CONTEXT & RESEARCH FRAMEWORK
- TODAY / CUTOFF: Treat today as 17 January 2026. Use and cite only sources published on or before 17 January 2026. If a source page is updated after that date, treat it as out of scope unless you can verify the relevant content existed on or before 17 January 2026.
- Output Language: Write the entire report in Polish.
- Product Context: Polish Tax Advisor GPT — decision-support for licensed Polish tax advisors in a regulated domain. The system must prioritize factual grounding, verifiable citations, and safe refusal when uncertainty is high.
- Expert(s) conducting the research: AI safety engineer specializing in hallucination prevention; RAG/knowledge engineer; evaluation engineer; Polish tax advisor SME; Responsible AI governance lead; SRE/MLOps reviewer for runtime monitoring.
- AI Domain: LLM factual grounding and hallucination mitigation for legal/regulatory tax content; production controls for Custom GPT deployments.
- Research Objective: Produce a production-ready accuracy control stack for Polish Tax Advisor GPT, including taxonomy of hallucination patterns, layered prevention techniques (data, prompt, retrieval, runtime, post-processing, human review), detection signals and monitoring metrics, and implementation templates/pseudocode that can be used to build and operate the system.
- Current AI Understanding: Assume the assistant uses knowledge and/or retrieval workflows; prior tiers defined scope and content sources. If unknown, state assumptions and provide options.
- Knowledge Gaps & Technical Uncertainties:
  - Common failure patterns in tax/legal reasoning, especially around amendments and citations
  - Which mitigations are reliable in production vs research-stage
  - How to implement runtime checks under Custom GPT constraints
  - How to express uncertainty to avoid misleading advisors

2. HYPOTHESIS & THEORETICAL FOUNDATION
- Primary AI Research Question: What techniques most effectively prevent hallucination and ensure factual accuracy for a tax advisory Custom GPT while preserving usability for professionals?
- Working Hypotheses to Verify:
  - H1: Mandatory citation and grounding instructions reduce hallucination in regulated domains
  - H2: Retrieval-augmented generation (RAG) reduces hallucination compared to pure generation for legal/tax questions
  - H3: Clear refusal and “I do not know” patterns improve safety and reduce fabricated answers
  - H4: Structured output templates reduce hallucination by constraining generation and forcing explicit evidence mapping
- Alternative Theories / Counterpoints:
  - RAG can introduce errors if retrieval is wrong or stale, leading to “grounded hallucinations”
  - Over-reliance on citations can hide misunderstandings if citations are irrelevant or misapplied
  - Detection methods may have high false positives/negatives; do not overstate capabilities
- Theoretical Basis: Research on LLM hallucinations, factuality evaluation, RAG reliability, uncertainty calibration, and safety controls in regulated settings.

3. METHODOLOGICAL PARAMETERS
- Engineering / Research Approach:
  - Evidence synthesis from peer-reviewed research on hallucinations and mitigation
  - Mapping techniques to layers: data/knowledge, retrieval, prompting, runtime validation, human review, monitoring
  - Define production-readiness criteria and failure modes for each technique
  - Propose an evaluation and regression-testing strategy for tax/legal accuracy
- Data & Evidence Required:
  - Peer-reviewed papers, benchmark studies, and authoritative engineering reports on hallucination reduction, RAG evaluation, uncertainty calibration
  - Official Custom GPT platform documentation constraints relevant to retrieval, citations, and tool use
  - Industry best practices for evaluation/monitoring of LLM products in regulated domains
- Timeframe: Up to 17 January 2026.
- Scope:
  - Tax/legal factual accuracy, legal basis referencing, numeric thresholds, effective dates, and procedural rules
  - Include multilingual and Polish-specific issues (Polish legal citations, Polish language ambiguity)
- Ethics & Governance:
  - Ensure design discourages collection of sensitive client data
  - Include safety boundaries and refusal triggers
  - Avoid overstating detection accuracy; explicitly document limitations

4. OUTPUT SPECIFICATIONS
- Report Architecture: Production accuracy control stack specification + evaluation plan + monitoring and incident signals for Polish Tax Advisor GPT.
- Abstract Required: Yes.
- Analytical Depth:
  - [ ] Level 1: Short technical note (2–4 pages)
  - [ ] Level 2: Full report (8–12 pages)
  - [X] Level 3: Comprehensive engineering report with implementation templates, pseudocode, and operational metrics
- Required Elements (deliver in Polish; use tables/checklists; include citations):
  - [X] Hallucination Pattern Taxonomy (table):
      - Wzorzec | Opis | Przykład w doradztwie podatkowym | Przyczyna | Sygnały ostrzegawcze | Sposób wykrycia | Mitigacje
      - Include patterns specific to legal domains: invented articles, wrong amendment state, fabricated case signatures, mixing jurisdictions, obsolete thresholds, incorrect procedural steps
  - [X] Prevention Techniques Stack (table):
      - Warstwa | Technika | Jak wdrożyć w Custom GPT | Wymagane dane/narzędzia | Dowody skuteczności | Ryzyka uboczne | Status (badania / produkcja)
      - Must cover: knowledge curation/versioning, retrieval design, prompt constraints, structured templates, refusal rules, citation enforcement, tool-based verification, human review workflows
  - [X] Uncertainty Expression Framework:
      - calibrated confidence categories and Polish language patterns
      - explicit rules for “cannot answer safely” and how to propose verification steps
      - examples of safe phrasing and what to avoid
  - [X] Detection Methods (table):
      - Metoda | Co wykrywa | Wymagania | Automatyzacja | Typowe błędy | Jak mierzyć skuteczność
      - Include and compare: citation coverage checks, evidence-to-claim alignment heuristics, contradiction checks, anomaly detection, review sampling, automated link/id validation
  - [X] Evaluation and Regression Test Plan:
      - how to build a golden set of tax scenarios
      - scoring rubric for factuality and citation correctness
      - thresholds for release gating
      - plan for periodic re-testing after knowledge updates or instruction changes
  - [X] Monitoring Dashboard Specification:
      - metrics for hallucination rates and leading indicators
      - suggested alert thresholds and escalation
      - recommended segmentation (topic, user intent, source type)
  - [X] Implementation Artifacts:
      - copy-ready instruction modules for grounding and refusal behavior in Polish
      - pseudocode or structured logic for runtime checks (pre-response and post-response)
      - sample JSON schema for logging evaluation events and user feedback signals
  - [X] Confidence Assessment:
      - clearly separate what is evidence-backed vs expert judgment
      - list risks not fully solvable with current methods and the compensating controls
- Visualization Requirements: tables, decision trees, and checklists in text form; include a clear “stack diagram” in ASCII if helpful.
- Target Audience: Engineering, product, safety/compliance, and tax SMEs.
- Citation Requirements: Cite research for mitigation claims; cite platform docs for constraints; use consistent numeric citations plus bibliography.

5. EVIDENCE HIERARCHY & SOURCE QUALITY
- Source Priority:
  - Tier 1: Peer-reviewed studies and benchmark evaluations, standards/guidelines on AI safety and uncertainty, official platform documentation
  - Tier 2: High-quality engineering reports and industry papers with data
  - Tier 3: Practitioner blogs only for context and clearly labeled
- Source Exclusions: Marketing claims, unverifiable performance assertions, post-cutoff updates.

6. QUALITY ASSURANCE & REPRODUCIBILITY
- Evidence Grading: For each technique, rate evidence strength and production maturity.
- Reproducibility Requirements: Provide concrete implementation steps, templates, and acceptance criteria; avoid vague guidance.
- Bias Assessment: Address risks of “citation bias,” selective retrieval, and over-refusal.
- Uncertainty Quantification: Provide explicit limitations and where human review is mandatory.

7. RESEARCH VALIDATION PROTOCOL
- Negative Result Analysis: Describe conditions where mitigations fail or introduce new risks (grounded hallucinations, stale sources).
- Alternative Solution Comparison: Compare at least two retrieval strategies and two validation strategies with pros/cons.
- Peer Review Simulation: Add critique from a skeptical tax SME and an AI safety reviewer.
- Replication Strategy: Provide a roadmap for iterating controls based on real-world feedback and monitored metrics.


====================


PROMPT 4 (REFINED, Tier 6 / GPT Engineering): Deployment & Monitoring

1. TECHNICAL CONTEXT & FRAMEWORK
- TODAY / CUTOFF: Treat today as 17 January 2026. Use and cite only sources published on or before 17 January 2026. If a source page is updated after that date, treat it as out of scope unless you can verify the relevant content existed on or before 17 January 2026.
- Output Language: Write the entire report in Polish.
- Product Context: Polish Tax Advisor GPT — a production Custom GPT for licensed Polish tax advisors. The deployment must be reliable, auditable, secure, and maintain stable quality across updates.
- Expert(s) conducting the research: Senior MLOps engineer for LLM systems; Site Reliability Engineer; security/privacy engineer (GDPR); compliance/risk lead for regulated products; knowledge engineer; Polish tax advisor SME.
- Technical Domain: Production deployment, observability, feedback loops, change management, and incident response for a Custom GPT-based product, potentially including external actions/backends.
- Research Objective: Provide an implementation-ready deployment and monitoring playbook tailored to Custom GPT constraints, including readiness checklist, monitoring/alerting design, feedback ingestion and triage workflows, incident response runbooks, and an iteration cadence with regression testing and rollback guidance.
- Current System Understanding:
  - Assume the system is built using the Custom GPT builder with instructions and a curated knowledge base
  - Optional: actions that call an external backend for retrieval, validation, or telemetry
  - If architecture details are missing, propose two reference architectures: builder-only and builder plus actions/backend
- Knowledge Gaps & Opportunity:
  - Which quality gates are needed for a regulated professional assistant
  - How to monitor quality degradation without full internal model observability
  - How to implement operational controls within Custom GPT constraints
  - How to operationalize feedback from expert users into safe iterative improvements
- Potential Impact: Reduced outages and high-severity incidents, faster detection of quality regressions, safer updates, and improved user trust.

2. CORE TECHNICAL QUESTION & HYPOTHESIS
- Primary Technical Question: What deployment and monitoring practices ensure reliable production operation for a professional Custom GPT used by licensed tax advisors?
- Working Hypotheses to Verify:
  - H1: Staged rollout (beta to limited to general) reduces risk and improves learning
  - H2: Conversation-level quality metrics detect issues earlier than aggregate usage metrics
  - H3: Response-level feedback with structured reason codes is more actionable than general ratings
  - H4: Knowledge base updates require full regression testing due to citation and grounding risks
- Alternative Solutions / Counterpoints:
  - Excessive gating slows iteration and reduces product value
  - Some metrics can be gamed or correlate poorly with real quality
  - Operational constraints of Custom GPT may require external tooling that increases complexity and compliance scope

3. METHODOLOGICAL PARAMETERS
- Design / Assessment Approach:
  - Synthesize MLOps and SRE best practices for LLM products
  - Map practices to Custom GPT constraints and capabilities with citations to official platform documentation
  - Produce reference architectures and operational workflows that are implementable
  - Provide a concrete checklist and runbooks, not only principles
- Data Types:
  - Release artifacts: instructions, knowledge base versions, action schemas
  - Observability data: structured feedback, error logs from actions/backends, sampling-based expert reviews
  - Evaluation data: regression test suites and golden scenarios
- Temporal Scope: Up to 17 January 2026.
- Scope of Systems:
  - The Custom GPT configuration lifecycle: design, build, test, release, monitor, iterate
  - Optional external components: retrieval service, telemetry pipeline, feedback database, evaluation harness
- Compliance & Safety: GDPR, confidentiality expectations for advisor workflows, data minimization, retention policies, secure handling of any logged content.

4. OUTPUT SPECIFICATIONS
- Report Structure: Production deployment and monitoring playbook for Polish Tax Advisor GPT with checklists, templates, and runbooks.
- Abstract Required: Yes.
- Analytical Depth:
  - [ ] Level 1: Executive summary
  - [ ] Level 2: Full assessment (with diagrams)
  - [X] Level 3: Comprehensive report with appendices, copy-ready checklists, and operational templates
- Required Elements (must be delivered in Polish, with citations and implementation specificity):
  - [X] Reference Deployment Architectures:
      - builder-only architecture: capabilities, limitations, risk profile
      - builder plus actions/backends architecture: components, data flows, security boundaries
      - include an ASCII diagram for each and a table of pros/cons
  - [X] Pre-Deployment Checklist (30–40 items minimum):
      - quality gates, safety and compliance checks, documentation, approvals/sign-offs, evaluation results, rollback readiness
      - each checklist item must include: description, owner role, evidence required, pass/fail criteria
  - [X] Monitoring Framework (table):
      - Metryka | Co mierzy | Jak zbierać | Próg alertu | Częstotliwość | Reakcja | Właściciel
      - include reliability metrics (latency, errors), quality metrics (citation coverage, refusal rates, groundedness proxies), safety metrics (PII leakage signals), and drift indicators
  - [X] Feedback Collection and Triage Design:
      - in-conversation feedback UX (structured reason codes)
      - periodic expert review workflow
      - taxonomy of issue types and routing rules
      - backlog integration and prioritization
  - [X] Incident Response Playbook (table):
      - Typ incydentu | Poziom (S1–S4) | Sygnały wykrycia | Natychmiastowe działania | Komunikacja | Naprawa trwała | Weryfikacja po naprawie
      - include: hallucination spike, wrong law version, broken citations, action outage, prompt injection, privacy incident
  - [X] Iteration and Release Process:
      - cadence options (weekly, biweekly, monthly) with pros/cons
      - testing requirements by update type (instruction change, knowledge update, action/backend change)
      - rollback plan and “stop the line” criteria
      - change log and release notes template in Polish
  - [X] Tooling Requirements and Gaps:
      - which practices require custom tooling outside the builder
      - minimal viable tooling stack for production operation
      - data retention and access controls for logs and feedback
  - [X] Confidence Assessment:
      - evidence-backed vs rationale-based guidance
      - explicit open risks and mitigation plan
- Visualization Requirements: architecture diagrams in ASCII, tables, checklists, decision trees, runbooks.
- Target Audience: Engineering, operations, product leadership, compliance/risk, and tax SMEs.
- Citation Requirements:
  - Cite official Custom GPT / OpenAI documentation for platform constraints and operational assumptions
  - Cite SRE/MLOps and LLM-ops best practices for monitoring and incident response guidance
  - Use consistent numeric citations and a bibliography

5. EVIDENCE HIERARCHY & SOURCE QUALITY
- Source Priority:
  - Tier 1: Official platform documentation and standards bodies; authoritative SRE/MLOps references; peer-reviewed or data-backed LLM operations research
  - Tier 2: High-quality industry playbooks with transparent methodology
  - Tier 3: Practitioner materials for context only
- Source Exclusions: Marketing-only claims, unverifiable operational recommendations, post-cutoff updates.

6. QUALITY ASSURANCE & REPRODUCIBILITY
- Evidence Grading: Tag each recommended practice with evidence strength and implementation complexity.
- Reproducibility Requirements: Provide copy-ready checklists and templates; define acceptance criteria and owners.
- Bias Assessment: Identify risks of metric gaming, Goodhart effects, and over-reliance on proxy metrics.
- Uncertainty Quantification: Clearly state what cannot be observed directly within Custom GPT constraints and propose compensating controls.

7. RESEARCH VALIDATION PROTOCOL
- Negative Result Consideration: Document failure modes where monitoring misses issues and how to mitigate (sampling, expert review).
- Alternative Solution Analysis: Compare at least two monitoring designs and two feedback-loop designs with trade-offs.
- Peer Review Simulation: Include critique from SRE, compliance/privacy, and tax SME perspectives.
- Replication Strategy: Provide a schedule for periodic audits, tabletop incident exercises, and quarterly review of SLOs and quality gates.
