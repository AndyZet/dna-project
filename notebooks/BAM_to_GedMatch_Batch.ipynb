{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# BAM to GedMatch Batch Conversion (Google Colab)\n\n**Convert Bodzia cemetery BAM files to GedMatch-compatible 23andMe format.**\n\n## Priority Samples (Grave E864/I)\n\n| Sample | Identity | Drive Link |\n|--------|----------|------------|\n| **VK155** | Bodzia female (mtDNA H1c) | [Download](https://drive.google.com/file/d/1H03qH351o_RemyhZeSf2euICP-OE4Spw/view) |\n| **VK157** | Bodzia elite male (Y-DNA I-S2077) | [Download](https://drive.google.com/file/d/1qK86VzMrAA_pzf1okvkjCgW5q2_XPAZS/view) |\n\n**Runtime:** ~15-30 min per sample after reference download (~45 min one-time)\n\n**Source folder:** [Google Drive](https://drive.google.com/drive/folders/1X7XTgWEF7h95QfJbv4tOo493O68Mo9UH)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install bioinformatics tools\n",
    "!apt-get update -qq\n",
    "!apt-get install -qq bcftools samtools tabix plink1.9\n",
    "!ln -sf /usr/bin/plink1.9 /usr/bin/plink\n",
    "\n",
    "print(\"Tools installed:\")\n",
    "!bcftools --version | head -1\n",
    "!samtools --version | head -1\n",
    "!plink --version | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download reference genome (one-time, ~10 min)\n",
    "import os\n",
    "\n",
    "REF_GENOME = \"/content/reference/human_g1k_v37.fasta\"\n",
    "\n",
    "if not os.path.exists(REF_GENOME):\n",
    "    print(\"Downloading GRCh37 reference genome (~3GB)...\")\n",
    "    !mkdir -p /content/reference\n",
    "    !wget -q --show-progress ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/human_g1k_v37.fasta.gz -O /content/reference/human_g1k_v37.fasta.gz\n",
    "    print(\"Decompressing...\")\n",
    "    !gunzip -f /content/reference/human_g1k_v37.fasta.gz\n",
    "    print(\"Indexing...\")\n",
    "    !samtools faidx /content/reference/human_g1k_v37.fasta\n",
    "    print(\"‚úì Reference genome ready!\")\n",
    "else:\n",
    "    print(\"‚úì Reference genome already present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Download dbSNP (one-time, ~30 min)\n",
    "DBSNP = \"/content/dbsnp/All_20180423.vcf.gz\"\n",
    "\n",
    "if not os.path.exists(DBSNP):\n",
    "    print(\"Downloading dbSNP b151 (~10GB)...\")\n",
    "    !mkdir -p /content/dbsnp\n",
    "    !wget -q --show-progress ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/GATK/All_20180423.vcf.gz -O /content/dbsnp/All_20180423.vcf.gz\n",
    "    !wget -q ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/GATK/All_20180423.vcf.gz.tbi -O /content/dbsnp/All_20180423.vcf.gz.tbi\n",
    "    print(\"‚úì dbSNP ready!\")\n",
    "else:\n",
    "    print(\"‚úì dbSNP already present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Configure BAM files\nimport os\nimport subprocess\n\n# Output locations\nDRIVE_OUTPUT = \"/content/drive/MyDrive/GedMatch_Files\"\nOUTPUT_DIR = \"/content/output\"\nBAM_DIR = \"/content/bam_files\"\n\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nos.makedirs(DRIVE_OUTPUT, exist_ok=True)\nos.makedirs(BAM_DIR, exist_ok=True)\n\n# ============================================================\n# PRIORITY SAMPLES - Direct Google Drive Links\n# ============================================================\nPRIORITY_FILES = {\n    \"VK155\": \"1H03qH351o_RemyhZeSf2euICP-OE4Spw\",  # Bodzia female (mtDNA H1c)\n    \"VK157\": \"1qK86VzMrAA_pzf1okvkjCgW5q2_XPAZS\",  # Bodzia elite male (Y-DNA I-S2077)\n}\n\n# Download priority BAM files directly from Drive\nprint(\"üì• Downloading priority BAM files from Google Drive...\\n\")\n\nbam_files = []\nfor sample, file_id in PRIORITY_FILES.items():\n    local_path = f\"{BAM_DIR}/{sample}.final.bam\"\n    \n    if os.path.exists(local_path):\n        print(f\"‚úì {sample}.final.bam already downloaded\")\n    else:\n        print(f\"‚¨áÔ∏è  Downloading {sample}.final.bam...\")\n        # Use gdown for direct Drive download\n        !pip install -q gdown\n        import gdown\n        url = f\"https://drive.google.com/uc?id={file_id}\"\n        gdown.download(url, local_path, quiet=False)\n        print(f\"‚úì {sample}.final.bam downloaded\")\n    \n    if os.path.exists(local_path):\n        size_gb = os.path.getsize(local_path) / (1024**3)\n        print(f\"   Size: {size_gb:.2f} GB\")\n        bam_files.append(local_path)\n\nprint(f\"\\nüìä Ready to process {len(bam_files)} priority samples\")\nprint(f\"üíæ Output will be saved to: {DRIVE_OUTPUT}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define batch processing function\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def convert_bam_to_gedmatch(bam_path, ref_genome, dbsnp, output_dir, drive_output):\n",
    "    \"\"\"Convert a single BAM file to GedMatch 23andMe format.\"\"\"\n",
    "    sample = os.path.basename(bam_path).replace('.final.bam', '')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {sample}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if already processed\n",
    "        final_output = f\"{drive_output}/{sample}_23andme.txt\"\n",
    "        if os.path.exists(final_output):\n",
    "            print(f\"‚è≠Ô∏è  Skipping {sample} - already processed\")\n",
    "            return {\"sample\": sample, \"status\": \"skipped\", \"snps\": 0}\n",
    "        \n",
    "        # Index BAM if needed\n",
    "        if not os.path.exists(f\"{bam_path}.bai\"):\n",
    "            print(f\"[1/6] Indexing BAM...\")\n",
    "            subprocess.run([\"samtools\", \"index\", bam_path], check=True)\n",
    "        else:\n",
    "            print(f\"[1/6] BAM index exists ‚úì\")\n",
    "        \n",
    "        # Variant calling\n",
    "        print(f\"[2/6] Variant calling (chr 1-22)...\")\n",
    "        raw_vcf = f\"{output_dir}/{sample}_raw.vcf.gz\"\n",
    "        mpileup = subprocess.Popen(\n",
    "            [\"bcftools\", \"mpileup\", \"-Ou\", \"-f\", ref_genome,\n",
    "             \"-r\", \"1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22\",\n",
    "             \"--max-depth\", \"250\", \"--min-MQ\", \"20\", bam_path],\n",
    "            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        subprocess.run(\n",
    "            [\"bcftools\", \"call\", \"-mv\", \"-Oz\", \"-o\", raw_vcf],\n",
    "            stdin=mpileup.stdout, check=True, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        subprocess.run([\"bcftools\", \"index\", raw_vcf], check=True)\n",
    "        \n",
    "        # Filter\n",
    "        print(f\"[3/6] Filtering (QUAL >= 20)...\")\n",
    "        filtered_vcf = f\"{output_dir}/{sample}_filtered.vcf.gz\"\n",
    "        subprocess.run(\n",
    "            [\"bcftools\", \"filter\", \"-i\", \"QUAL>=20\", raw_vcf, \"-Oz\", \"-o\", filtered_vcf],\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        # Normalize\n",
    "        print(f\"[4/6] Normalizing...\")\n",
    "        normalized_vcf = f\"{output_dir}/{sample}_normalized.vcf.gz\"\n",
    "        subprocess.run(\n",
    "            [\"bcftools\", \"norm\", \"-f\", ref_genome, filtered_vcf, \"-Oz\", \"-o\", normalized_vcf],\n",
    "            check=True, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        subprocess.run([\"bcftools\", \"index\", normalized_vcf], check=True)\n",
    "        \n",
    "        # Annotate with rsIDs\n",
    "        print(f\"[5/6] Annotating with rsIDs...\")\n",
    "        annotated_vcf = f\"{output_dir}/{sample}_annotated.vcf.gz\"\n",
    "        subprocess.run(\n",
    "            [\"bcftools\", \"annotate\", \"-a\", dbsnp, \"-c\", \"ID\", normalized_vcf, \"-Oz\", \"-o\", annotated_vcf],\n",
    "            check=True, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        subprocess.run([\"bcftools\", \"index\", annotated_vcf], check=True)\n",
    "        \n",
    "        # Convert to 23andMe format\n",
    "        print(f\"[6/6] Converting to 23andMe format...\")\n",
    "        snps_vcf = f\"{output_dir}/{sample}_snps.vcf.gz\"\n",
    "        subprocess.run(\n",
    "            [\"bcftools\", \"view\", \"-i\", 'ID!=\".\" && strlen(REF)==1 && strlen(ALT)==1',\n",
    "             annotated_vcf, \"-Oz\", \"-o\", snps_vcf],\n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        plink_out = f\"{output_dir}/{sample}_plink\"\n",
    "        subprocess.run(\n",
    "            [\"plink\", \"--vcf\", snps_vcf, \"--recode\", \"23\", \"--chr\", \"1-22\", \"--out\", plink_out],\n",
    "            check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        \n",
    "        # Create final output\n",
    "        local_output = f\"{output_dir}/{sample}_23andme.txt\"\n",
    "        with open(local_output, 'w') as out:\n",
    "            out.write(\"# rsid\\tchromosome\\tposition\\tgenotype\\n\")\n",
    "            with open(f\"{plink_out}.txt\", 'r') as inp:\n",
    "                next(inp)  # Skip header\n",
    "                for line in inp:\n",
    "                    out.write(line)\n",
    "        \n",
    "        # Count SNPs\n",
    "        with open(local_output, 'r') as f:\n",
    "            snp_count = sum(1 for _ in f) - 1  # Subtract header\n",
    "        \n",
    "        # Copy to Drive\n",
    "        subprocess.run([\"cp\", local_output, final_output], check=True)\n",
    "        \n",
    "        # Cleanup intermediate files\n",
    "        for f in [raw_vcf, f\"{raw_vcf}.csi\", filtered_vcf, normalized_vcf, f\"{normalized_vcf}.csi\",\n",
    "                  annotated_vcf, f\"{annotated_vcf}.csi\", snps_vcf,\n",
    "                  f\"{plink_out}.txt\", f\"{plink_out}.log\", f\"{plink_out}.nosex\", local_output]:\n",
    "            if os.path.exists(f):\n",
    "                os.remove(f)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚úì {sample} complete: {snp_count:,} SNPs in {elapsed/60:.1f} min\")\n",
    "        \n",
    "        return {\"sample\": sample, \"status\": \"success\", \"snps\": snp_count, \"time\": elapsed}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚úó {sample} FAILED: {str(e)}\")\n",
    "        return {\"sample\": sample, \"status\": \"failed\", \"error\": str(e)}\n",
    "\n",
    "print(\"Batch processing function defined ‚úì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Run batch processing\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(f\"# BATCH PROCESSING STARTED: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"# Samples to process: {len(bam_files)}\")\n",
    "print(f\"{'#'*60}\")\n",
    "\n",
    "results = []\n",
    "total_start = time.time()\n",
    "\n",
    "for i, bam_file in enumerate(sorted(bam_files), 1):\n",
    "    print(f\"\\n[{i}/{len(bam_files)}] \", end=\"\")\n",
    "    result = convert_bam_to_gedmatch(\n",
    "        bam_file, REF_GENOME, DBSNP, OUTPUT_DIR, DRIVE_OUTPUT\n",
    "    )\n",
    "    results.append(result)\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "print(f\"\\n\\n{'#'*60}\")\n",
    "print(f\"# BATCH PROCESSING COMPLETE\")\n",
    "print(f\"# Total time: {total_elapsed/60:.1f} minutes\")\n",
    "print(f\"{'#'*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Summary report\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONVERSION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "successful = [r for r in results if r[\"status\"] == \"success\"]\n",
    "skipped = [r for r in results if r[\"status\"] == \"skipped\"]\n",
    "failed = [r for r in results if r[\"status\"] == \"failed\"]\n",
    "\n",
    "print(f\"\\n‚úì Successful: {len(successful)}\")\n",
    "print(f\"‚è≠Ô∏è  Skipped:    {len(skipped)}\")\n",
    "print(f\"‚úó Failed:     {len(failed)}\")\n",
    "\n",
    "if successful:\n",
    "    print(f\"\\n{'Sample':<12} {'SNPs':>12} {'Time (min)':>12} {'Quality':>15}\")\n",
    "    print(\"-\" * 55)\n",
    "    for r in sorted(successful, key=lambda x: x[\"snps\"], reverse=True):\n",
    "        quality = \"Good\" if r[\"snps\"] >= 200000 else \"Low (aDNA)\" if r[\"snps\"] >= 50000 else \"Very Low\"\n",
    "        print(f\"{r['sample']:<12} {r['snps']:>12,} {r['time']/60:>12.1f} {quality:>15}\")\n",
    "    \n",
    "    total_snps = sum(r[\"snps\"] for r in successful)\n",
    "    avg_snps = total_snps / len(successful)\n",
    "    print(\"-\" * 55)\n",
    "    print(f\"{'Average':<12} {avg_snps:>12,.0f}\")\n",
    "\n",
    "if failed:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed samples:\")\n",
    "    for r in failed:\n",
    "        print(f\"  ‚Ä¢ {r['sample']}: {r.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(f\"\\n\\nOutput files saved to: {DRIVE_OUTPUT}\")\n",
    "print(\"\\nTo upload to GedMatch:\")\n",
    "print(\"  1. Go to https://www.gedmatch.com\")\n",
    "print(\"  2. Log in ‚Üí DNA Upload\")\n",
    "print(\"  3. Select '23andMe' format\")\n",
    "print(f\"  4. Upload files from: GedMatch_Files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: List generated files\n",
    "print(\"\\nGenerated GedMatch files:\")\n",
    "print(\"-\" * 50)\n",
    "!ls -lh \"{DRIVE_OUTPUT}\"/*.txt 2>/dev/null || echo \"No files found\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes on Ancient DNA Quality\n",
    "\n",
    "**Expected SNP counts for ancient DNA:**\n",
    "- Modern consumer tests: 600,000 - 700,000 SNPs\n",
    "- High-coverage ancient DNA (>10x): 200,000+ SNPs\n",
    "- Medium-coverage ancient DNA (2-10x): 100,000 - 200,000 SNPs\n",
    "- Low-coverage ancient DNA (<2x): 50,000 - 100,000 SNPs\n",
    "\n",
    "**GedMatch compatibility:**\n",
    "- Minimum ~50,000 SNPs for upload\n",
    "- Matches will be fewer and shorter segments\n",
    "- Most useful for population-level analysis (Admixture, Oracle)\n",
    "- One-to-one comparisons may show artificially high shared DNA due to missing data\n",
    "\n",
    "**Bodzia cemetery context:**\n",
    "- Samples are ~1,000 years old\n",
    "- Expected coverage: 0.5-3x\n",
    "- Typical SNP yield: 50,000 - 150,000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}